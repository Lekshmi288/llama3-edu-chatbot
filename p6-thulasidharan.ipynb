{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run orimport torch\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:08:37.054424Z","iopub.execute_input":"2025-05-03T14:08:37.054651Z","iopub.status.idle":"2025-05-03T14:08:38.873286Z","shell.execute_reply.started":"2025-05-03T14:08:37.054623Z","shell.execute_reply":"2025-05-03T14:08:38.872683Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Section 0: Setup","metadata":{}},{"cell_type":"code","source":"import torch\nprint(\"GPU available:\", torch.cuda.is_available())\nprint(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:08:38.874141Z","iopub.execute_input":"2025-05-03T14:08:38.874491Z","iopub.status.idle":"2025-05-03T14:08:42.406542Z","shell.execute_reply.started":"2025-05-03T14:08:38.874465Z","shell.execute_reply":"2025-05-03T14:08:42.405857Z"}},"outputs":[{"name":"stdout","text":"GPU available: True\nGPU name: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:08:57.012526Z","iopub.execute_input":"2025-05-03T14:08:57.012791Z","iopub.status.idle":"2025-05-03T14:08:57.027334Z","shell.execute_reply.started":"2025-05-03T14:08:57.012771Z","shell.execute_reply":"2025-05-03T14:08:57.026513Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fae844b6b68435f8370e24be3d027a9"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"!pip install bitsandbytes>=0.39.0\n!pip install --upgrade accelerate transformers datasets peft trl\n!pip install streamlit\n!npm install -g localtunnel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:09:01.403643Z","iopub.execute_input":"2025-05-03T14:09:01.403943Z","iopub.status.idle":"2025-05-03T14:10:34.277142Z","shell.execute_reply.started":"2025-05-03T14:09:01.403923Z","shell.execute_reply":"2025-05-03T14:10:34.276288Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nCollecting accelerate\n  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nCollecting transformers\n  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nCollecting datasets\n  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nCollecting peft\n  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\nCollecting trl\n  Downloading trl-0.17.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (14.0.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.17.0-py3-none-any.whl (348 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, transformers, datasets, accelerate, trl, peft\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.1\n    Uninstalling transformers-4.51.1:\n      Successfully uninstalled transformers-4.51.1\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.5.0\n    Uninstalling datasets-3.5.0:\n      Successfully uninstalled datasets-3.5.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.3.0\n    Uninstalling accelerate-1.3.0:\n      Successfully uninstalled accelerate-1.3.0\n  Attempting uninstall: peft\n    Found existing installation: peft 0.14.0\n    Uninstalling peft-0.14.0:\n      Successfully uninstalled peft-0.14.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.6.0 datasets-3.5.1 fsspec-2025.3.0 peft-0.15.2 transformers-4.51.3 trl-0.17.0\nCollecting streamlit\n  Downloading streamlit-1.45.0-py3-none-any.whl.metadata (8.9 kB)\nRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\nRequirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\nRequirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\nRequirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.3)\nRequirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\nRequirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (19.0.1)\nRequirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\nRequirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.1)\nRequirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\nRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\nRequirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.26.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.23->streamlit) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nDownloading streamlit-1.45.0-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pydeck, streamlit\nSuccessfully installed pydeck-0.9.1 streamlit-1.45.0\n\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K\nadded 22 packages in 2s\n\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K\n\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K3 packages are looking for funding\n\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K  run `npm fund` for details\n\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m New \u001b[31mmajor\u001b[39m version of npm available! \u001b[31m10.8.2\u001b[39m -> \u001b[34m11.3.0\u001b[39m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m Changelog: \u001b[34mhttps://github.com/npm/cli/releases/tag/v11.3.0\u001b[39m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m To update run: \u001b[4mnpm install -g npm@11.3.0\u001b[24m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!wget https://github.com/CS639-Data-Management-for-Data-Science/s25/raw/main/p6/transcripts.zip\n!unzip -o transcripts.zip -d transcripts/ ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:10:34.278968Z","iopub.execute_input":"2025-05-03T14:10:34.279265Z","iopub.status.idle":"2025-05-03T14:10:34.843528Z","shell.execute_reply.started":"2025-05-03T14:10:34.279236Z","shell.execute_reply":"2025-05-03T14:10:34.842829Z"}},"outputs":[{"name":"stdout","text":"--2025-05-03 14:10:34--  https://github.com/CS639-Data-Management-for-Data-Science/s25/raw/main/p6/transcripts.zip\nResolving github.com (github.com)... 140.82.116.3\nConnecting to github.com (github.com)|140.82.116.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/CS639-Data-Management-for-Data-Science/s25/main/p6/transcripts.zip [following]\n--2025-05-03 14:10:34--  https://raw.githubusercontent.com/CS639-Data-Management-for-Data-Science/s25/main/p6/transcripts.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 290933 (284K) [application/zip]\nSaving to: â€˜transcripts.zipâ€™\n\ntranscripts.zip     100%[===================>] 284.11K  --.-KB/s    in 0.02s   \n\n2025-05-03 14:10:34 (15.5 MB/s) - â€˜transcripts.zipâ€™ saved [290933/290933]\n\nArchive:  transcripts.zip\n   creating: transcripts/transcripts/\n  inflating: transcripts/__MACOSX/._transcripts  \n  inflating: transcripts/transcripts/23 en-English-CS639_ Elasticsearch geo queries + Kibana.txt  \n  inflating: transcripts/__MACOSX/transcripts/._23 en-English-CS639_ Elasticsearch geo queries + Kibana.txt  \n  inflating: transcripts/transcripts/14 en-English-CS639_ MongoDB on Docker.txt  \n  inflating: transcripts/__MACOSX/transcripts/._14 en-English-CS639_ MongoDB on Docker.txt  \n  inflating: transcripts/transcripts/.DS_Store  \n  inflating: transcripts/__MACOSX/transcripts/._.DS_Store  \n  inflating: transcripts/transcripts/11 en-English-CS639_ SQL Joins.txt  \n  inflating: transcripts/__MACOSX/transcripts/._11 en-English-CS639_ SQL Joins.txt  \n  inflating: transcripts/transcripts/16 en-English-CS639_ MongoDB Operators.txt  \n  inflating: transcripts/__MACOSX/transcripts/._16 en-English-CS639_ MongoDB Operators.txt  \n  inflating: transcripts/transcripts/7 en-English-CS639_ SQL on docker.txt  \n  inflating: transcripts/__MACOSX/transcripts/._7 en-English-CS639_ SQL on docker.txt  \n  inflating: transcripts/transcripts/12 en-English-CS639_ SQL window functions.txt  \n  inflating: transcripts/__MACOSX/transcripts/._12 en-English-CS639_ SQL window functions.txt  \n  inflating: transcripts/transcripts/2 en-English-CS639_ Deployment (Linux Shell).txt  \n  inflating: transcripts/__MACOSX/transcripts/._2 en-English-CS639_ Deployment (Linux Shell).txt  \n  inflating: transcripts/transcripts/4 en-English-CS639_ Docker.txt  \n  inflating: transcripts/__MACOSX/transcripts/._4 en-English-CS639_ Docker.txt  \n  inflating: transcripts/transcripts/21 en-English-CS639_ Elasticsearch API intro.txt  \n  inflating: transcripts/__MACOSX/transcripts/._21 en-English-CS639_ Elasticsearch API intro.txt  \n  inflating: transcripts/transcripts/6.2 en-English-SQL 1_ Creating tables (post fire-alarm).txt  \n  inflating: transcripts/__MACOSX/transcripts/._6.2 en-English-SQL 1_ Creating tables (post fire-alarm).txt  \n  inflating: transcripts/transcripts/1 en-English-CS639_ Course intro.txt  \n  inflating: transcripts/__MACOSX/transcripts/._1 en-English-CS639_ Course intro.txt  \n  inflating: transcripts/transcripts/17 en-English-CS639_ MongoDB Aggregation.txt  \n  inflating: transcripts/__MACOSX/transcripts/._17 en-English-CS639_ MongoDB Aggregation.txt  \n  inflating: transcripts/transcripts/20 en-English-CS639_ Elasticsearch intro.txt  \n  inflating: transcripts/__MACOSX/transcripts/._20 en-English-CS639_ Elasticsearch intro.txt  \n  inflating: transcripts/transcripts/22 en-English-CS639_ Elasticsearch_ Boosting, highlighting, and aggregations.txt  \n  inflating: transcripts/__MACOSX/transcripts/._22 en-English-CS639_ Elasticsearch_ Boosting, highlighting, and aggregations.txt  \n  inflating: transcripts/transcripts/6.1 en-English-CS639_ SQL 1_ Creating tables (part 1).txt  \n  inflating: transcripts/__MACOSX/transcripts/._6.1 en-English-CS639_ SQL 1_ Creating tables (part 1).txt  \n  inflating: transcripts/transcripts/13 en-English-CS639_ Non-relational databases_ MongoDB.txt  \n  inflating: transcripts/__MACOSX/transcripts/._13 en-English-CS639_ Non-relational databases_ MongoDB.txt  \n  inflating: transcripts/transcripts/15 en-English-CS639_ MongoDB API.txt  \n  inflating: transcripts/__MACOSX/transcripts/._15 en-English-CS639_ MongoDB API.txt  \n  inflating: transcripts/transcripts/10 en-English-CS639_ SQL subqueries.txt  \n  inflating: transcripts/__MACOSX/transcripts/._10 en-English-CS639_ SQL subqueries.txt  \n  inflating: transcripts/transcripts/8 en-English-CS639_ Relational Algebra (RA).txt  \n  inflating: transcripts/__MACOSX/transcripts/._8 en-English-CS639_ Relational Algebra (RA).txt  \n  inflating: transcripts/transcripts/3 en-English-CS639_ Deployment (Linux Pipelines).txt  \n  inflating: transcripts/__MACOSX/transcripts/._3 en-English-CS639_ Deployment (Linux Pipelines).txt  \n  inflating: transcripts/transcripts/5 en-English-CS639_ Relational Database Management Systems (RDBMS).txt  \n  inflating: transcripts/__MACOSX/transcripts/._5 en-English-CS639_ Relational Database Management Systems (RDBMS).txt  \n  inflating: transcripts/transcripts/18 en-English-CS639_ MongoDB Geospatial Operators.txt  \n  inflating: transcripts/__MACOSX/transcripts/._18 en-English-CS639_ MongoDB Geospatial Operators.txt  \n  inflating: transcripts/transcripts/9 en-English-CS639_ Basic SQL queries (partial lecture).txt  \n  inflating: transcripts/__MACOSX/transcripts/._9 en-English-CS639_ Basic SQL queries (partial lecture).txt  \n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# ğŸ“˜ Section 1: Text Generation with a Pre-Trained LLM","metadata":{"execution":{"iopub.status.busy":"2025-05-01T21:09:25.467671Z","iopub.execute_input":"2025-05-01T21:09:25.467949Z","iopub.status.idle":"2025-05-01T21:09:25.471705Z","shell.execute_reply.started":"2025-05-01T21:09:25.467929Z","shell.execute_reply":"2025-05-01T21:09:25.470890Z"}}},{"cell_type":"markdown","source":"Q1.1: Load a 4-bit quantized Llama-3.2-1B-Instruct model and and its tokenizer.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:10:34.844458Z","iopub.execute_input":"2025-05-03T14:10:34.844688Z","iopub.status.idle":"2025-05-03T14:10:39.867410Z","shell.execute_reply.started":"2025-05-03T14:10:34.844666Z","shell.execute_reply":"2025-05-03T14:10:39.866644Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model_id = \"meta-llama/Llama-3.2-1B-Instruct\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:10:39.869629Z","iopub.execute_input":"2025-05-03T14:10:39.870408Z","iopub.status.idle":"2025-05-03T14:10:39.874072Z","shell.execute_reply.started":"2025-05-03T14:10:39.870379Z","shell.execute_reply":"2025-05-03T14:10:39.873440Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",bnb_4bit_compute_dtype=torch.float16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:10:39.874743Z","iopub.execute_input":"2025-05-03T14:10:39.874952Z","iopub.status.idle":"2025-05-03T14:10:41.458450Z","shell.execute_reply.started":"2025-05-03T14:10:39.874936Z","shell.execute_reply":"2025-05-03T14:10:41.457611Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:10:41.459439Z","iopub.execute_input":"2025-05-03T14:10:41.460013Z","iopub.status.idle":"2025-05-03T14:10:43.244069Z","shell.execute_reply.started":"2025-05-03T14:10:41.459985Z","shell.execute_reply":"2025-05-03T14:10:43.243451Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72fdd3846584431a889681f7d2ec3b99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4a26faa82404568b75fec2573478a36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74077f7bab334ce99411d70111a13f1d"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(model_id,quantization_config=bnb_config,low_cpu_mem_usage=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:10:43.244829Z","iopub.execute_input":"2025-05-03T14:10:43.245083Z","iopub.status.idle":"2025-05-03T14:11:07.246107Z","shell.execute_reply.started":"2025-05-03T14:10:43.245057Z","shell.execute_reply":"2025-05-03T14:11:07.245504Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15ffaca50291409795b56338817e51d3"}},"metadata":{}},{"name":"stderr","text":"2025-05-03 14:10:45.559814: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746281445.750179      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746281445.802361      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"749f8f72792a4c739d85a9b1cfdb1511"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fad8fc65b344485cb62a4910e67b64ed"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"Q1.2: Test your quantized model with different prompts (text generation).","metadata":{}},{"cell_type":"code","source":"help(tokenizer.__call__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:11:07.246853Z","iopub.execute_input":"2025-05-03T14:11:07.247358Z","iopub.status.idle":"2025-05-03T14:11:07.252156Z","shell.execute_reply.started":"2025-05-03T14:11:07.247338Z","shell.execute_reply":"2025-05-03T14:11:07.251418Z"}},"outputs":[{"name":"stdout","text":"Help on method __call__ in module transformers.tokenization_utils_base:\n\n__call__(text: Union[str, List[str], List[List[str]], NoneType] = None, text_pair: Union[str, List[str], List[List[str]], NoneType] = None, text_target: Union[str, List[str], List[List[str]], NoneType] = None, text_pair_target: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy, NoneType] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, padding_side: Optional[str] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding method of transformers.tokenization_utils_fast.PreTrainedTokenizerFast instance\n    Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n    sequences.\n    \n    Args:\n        text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n        text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n        text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n        text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n    \n        add_special_tokens (`bool`, *optional*, defaults to `True`):\n            Whether or not to add special tokens when encoding the sequences. This will use the underlying\n            `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n            automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n            automatically.\n        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n            Activates and controls padding. Accepts the following values:\n    \n            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n              sequence if provided).\n            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n              acceptable input length for the model if that argument is not provided.\n            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n              lengths).\n        truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n            Activates and controls truncation. Accepts the following values:\n    \n            - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n              to the maximum acceptable input length for the model if that argument is not provided. This will\n              truncate token by token, removing a token from the longest sequence in the pair if a pair of\n              sequences (or a batch of pairs) is provided.\n            - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n              maximum acceptable input length for the model if that argument is not provided. This will only\n              truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n            - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n              maximum acceptable input length for the model if that argument is not provided. This will only\n              truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n            - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n              greater than the model maximum admissible input size).\n        max_length (`int`, *optional*):\n            Controls the maximum length to use by one of the truncation/padding parameters.\n    \n            If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n            is required by one of the truncation/padding parameters. If the model has no specific maximum input\n            length (like XLNet) truncation/padding to a maximum length will be deactivated.\n        stride (`int`, *optional*, defaults to 0):\n            If set to a number along with `max_length`, the overflowing tokens returned when\n            `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n            returned to provide some overlap between truncated and overflowing sequences. The value of this\n            argument defines the number of overlapping tokens.\n        is_split_into_words (`bool`, *optional*, defaults to `False`):\n            Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n            tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n            which it will tokenize. This is useful for NER or token classification.\n        pad_to_multiple_of (`int`, *optional*):\n            If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n            `>= 7.5` (Volta).\n        padding_side (`str`, *optional*):\n            The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n            Default value is picked from the class attribute of the same name.\n        return_tensors (`str` or [`~utils.TensorType`], *optional*):\n            If set, will return tensors instead of list of python integers. Acceptable values are:\n    \n            - `'tf'`: Return TensorFlow `tf.constant` objects.\n            - `'pt'`: Return PyTorch `torch.Tensor` objects.\n            - `'np'`: Return Numpy `np.ndarray` objects.\n    \n        return_token_type_ids (`bool`, *optional*):\n            Whether to return token type IDs. If left to the default, will return the token type IDs according to\n            the specific tokenizer's default, defined by the `return_outputs` attribute.\n    \n            [What are token type IDs?](../glossary#token-type-ids)\n        return_attention_mask (`bool`, *optional*):\n            Whether to return the attention mask. If left to the default, will return the attention mask according\n            to the specific tokenizer's default, defined by the `return_outputs` attribute.\n    \n            [What are attention masks?](../glossary#attention-mask)\n        return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n            Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n            of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n            of returning overflowing tokens.\n        return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n            Whether or not to return special tokens mask information.\n        return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n            Whether or not to return `(char_start, char_end)` for each token.\n    \n            This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n            Python's tokenizer, this method will raise `NotImplementedError`.\n        return_length  (`bool`, *optional*, defaults to `False`):\n            Whether or not to return the lengths of the encoded inputs.\n        verbose (`bool`, *optional*, defaults to `True`):\n            Whether or not to print more information and warnings.\n        **kwargs: passed to the `self.tokenize()` method\n    \n    Return:\n        [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n    \n        - **input_ids** -- List of token ids to be fed to a model.\n    \n          [What are input IDs?](../glossary#input-ids)\n    \n        - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n          if *\"token_type_ids\"* is in `self.model_input_names`).\n    \n          [What are token type IDs?](../glossary#token-type-ids)\n    \n        - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n          `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n    \n          [What are attention masks?](../glossary#attention-mask)\n    \n        - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n          `return_overflowing_tokens=True`).\n        - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n          `return_overflowing_tokens=True`).\n        - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n          regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n        - **length** -- The length of the inputs (when `return_length=True`)\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def generate_response(prompt, max_new_tokens):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:11:07.253068Z","iopub.execute_input":"2025-05-03T14:11:07.253330Z","iopub.status.idle":"2025-05-03T14:11:08.518577Z","shell.execute_reply.started":"2025-05-03T14:11:07.253309Z","shell.execute_reply":"2025-05-03T14:11:08.517887Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"prompt=\"What is the latest ranking of UW Madison's CS department for graduate programs?\"\nresponse = generate_response(prompt, max_new_tokens=300)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:11:08.520780Z","iopub.execute_input":"2025-05-03T14:11:08.521252Z","iopub.status.idle":"2025-05-03T14:11:15.392756Z","shell.execute_reply.started":"2025-05-03T14:11:08.521231Z","shell.execute_reply":"2025-05-03T14:11:15.391973Z"}},"outputs":[{"name":"stdout","text":"What is the latest ranking of UW Madison's CS department for graduate programs? (2023 or 2024)\nI'm trying to find the most up-to-date information on the latest ranking of UW Madison's CS department for graduate programs. I've tried searching for it on the department's website or other reputable sources, but I couldn't find the answer.\n\nTo help you, I'll provide some information on the latest rankings:\n\n* According to the US News & World Report (2023) and the National Science Foundation (2024) rankings, UW Madison's Computer Science (CS) department is ranked #5 in the nation for graduate programs.\n* The Computer Science department at UW Madison is known for its strong research programs, innovative curriculum, and faculty expertise in areas like artificial intelligence, machine learning, and cybersecurity.\n* The department has a strong reputation for producing well-rounded graduates who are prepared for careers in a wide range of fields, including tech, healthcare, finance, and more.\n\nPlease let me know if I'm correct or if I need to dig deeper. I'll do my best to find the most up-to-date information.\n\nReferences:\n\n* US News & World Report (2023) - Computer Science Department Rankings\n* National Science Foundation (2024) - Computer Science Department Rankings\n* UW Madison Computer Science Department website (no update available yet)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"prompt=\"How to become a data scientist with a Physics PhD degree?\"\nresponse = generate_response(prompt, max_new_tokens=800)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:11:15.393444Z","iopub.execute_input":"2025-05-03T14:11:15.393645Z","iopub.status.idle":"2025-05-03T14:11:32.891672Z","shell.execute_reply.started":"2025-05-03T14:11:15.393628Z","shell.execute_reply":"2025-05-03T14:11:32.890859Z"}},"outputs":[{"name":"stdout","text":"How to become a data scientist with a Physics PhD degree? A challenging but achievable goal!\n\nWhile a Physics PhD degree may seem like an insurmountable obstacle, it's not impossible to become a data scientist. Here's a step-by-step guide to help you achieve your goal:\n\n**Step 1: Understand the basics of data science**\n\nStart by learning the fundamentals of data science, including:\n\n* Data structures and algorithms\n* Machine learning and deep learning\n* Statistical inference and modeling\n* Data visualization and communication\n* Domain-specific knowledge (e.g., medical imaging, finance, or social media)\n\n**Step 2: Gain relevant experience and skills**\n\n1. **Take online courses**: Websites like Coursera, edX, and Udemy offer courses on data science, machine learning, and statistics.\n2. **Practice with projects**: Apply your knowledge to real-world projects, such as:\n\t* Predicting stock prices using historical data and machine learning\n\t* Building a recommender system for movie or book recommendations\n\t* Analyzing and visualizing data from various domains (e.g., climate, economics, or social media)\n3. **Join online communities**: Participate in online forums, such as Kaggle, Reddit's r/MachineLearning, and Data Science Stack Exchange.\n4. **Read books and research papers**: Stay up-to-date with the latest research and books on data science.\n\n**Step 3: Develop a strong foundation in programming**\n\n1. **Learn a programming language**: Focus on Python, R, or SQL as your primary language.\n2. **Master machine learning libraries**: Familiarize yourself with popular libraries like scikit-learn, TensorFlow, or PyTorch.\n3. **Practice with datasets**: Use public datasets to train and test models.\n\n**Step 4: Pursue a graduate degree (optional)**\n\nIf you want to specialize in a specific domain, such as machine learning or data visualization, consider pursuing a graduate degree (e.g., Master's or Ph.D.) in data science.\n\n**Step 5: Build a strong portfolio**\n\nCreate a portfolio showcasing your projects, skills, and achievements. This will help you stand out to potential employers or collaborators.\n\n**Step 6: Network and find a mentor**\n\nAttend conferences, meetups, or online events to connect with professionals in the field. Find a mentor who can guide you and provide valuable insights.\n\n**Step 7: Stay up-to-date with industry trends**\n\nContinuously update your knowledge on emerging technologies, such as:\n\n* Natural Language Processing (NLP)\n* Computer Vision\n* Reinforcement Learning\n* Deep learning\n* Cloud computing and big data\n\n**Step 8: Prepare for job applications**\n\nUpdate your resume and cover letter to highlight your skills, experience, and achievements. Prepare for common data science interview questions.\n\n**Challenges and obstacles:**\n\n1. **Physics PhD requirements**: While you may have a Physics PhD, it's not a requirement for most data science roles. Focus on developing your data science skills and experience.\n2. **Steep learning curve**: Data science is a vast field, and it may take time to absorb new concepts and technologies.\n3. **Competition**: The data science job market is highly competitive, and it's essential to differentiate yourself from others.\n\n**Tips and advice:**\n\n* **Focus on problem-solving**: Instead of just memorizing formulas, focus on solving real-world problems.\n* **Be curious and open-minded**: Data science is a field that values experimentation and creativity.\n* **Join online communities**: Engage with online forums to learn from others and get feedback on your projects.\n* **Take breaks and stay motivated**: Data science can be mentally demanding, so make sure to take breaks and stay motivated.\n\nBecoming a data scientist with a Physics PhD degree is a challenging but achievable goal. By following these steps, you'll be well on your way to developing the skills and experience needed to succeed in this field. Good luck!\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"prompt=\"Heard that the job market for Data science is quite saturated right now. What are some projects which could help my portfolio stand out?\"\nresponse = generate_response(prompt, max_new_tokens=700)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:12:00.333126Z","iopub.execute_input":"2025-05-03T14:12:00.333725Z","iopub.status.idle":"2025-05-03T14:12:12.222393Z","shell.execute_reply.started":"2025-05-03T14:12:00.333681Z","shell.execute_reply":"2025-05-03T14:12:12.221757Z"}},"outputs":[{"name":"stdout","text":"Heard that the job market for Data science is quite saturated right now. What are some projects which could help my portfolio stand out? Here are some projects that I can consider:\n\n1. **Predictive Modeling Project**: Create a predictive model that can forecast stock prices or any other financial data. You can use libraries like scikit-learn, TensorFlow, or PyTorch to build the model.\n\n2. **Recommendation System**: Develop a recommendation system that suggests products or services based on user behavior. This can be a fun project to work on, as it involves understanding user preferences and behavior.\n\n3. **Image Classification Project**: Use a deep learning model to classify images into different categories. This can be a fun project to work on, as it involves understanding the structure of images and how to classify them.\n\n4. **Natural Language Processing (NLP) Project**: Develop a project that uses NLP to analyze and understand user input. This can be a fun project to work on, as it involves understanding the structure of text and how to analyze it.\n\n5. **Chatbot Project**: Create a chatbot that can engage in natural language conversations with users. This can be a fun project to work on, as it involves understanding the structure of language and how to build a conversational interface.\n\n6. **Time Series Analysis Project**: Use a time series analysis library like statsmodels or pandas to analyze and forecast time series data. This can be a fun project to work on, as it involves understanding the structure of time series data and how to forecast it.\n\n7. **Computer Vision Project**: Use a deep learning model to analyze and understand images. This can be a fun project to work on, as it involves understanding the structure of images and how to analyze them.\n\n8. **Generative Models Project**: Develop a project that uses generative models like GANs or VAEs to generate new data. This can be a fun project to work on, as it involves understanding the structure of data and how to generate new data.\n\n9. **Data Enrichment Project**: Use data enrichment techniques like data profiling, data curation, and data quality improvement to improve the quality of your data. This can be a fun project to work on, as it involves understanding the importance of data quality and how to improve it.\n\n10. **Big Data Analytics Project**: Use big data analytics libraries like Apache Spark or Hadoop to analyze and understand large datasets. This can be a fun project to work on, as it involves understanding the structure of big data and how to analyze it.\n\nRemember, the key to standing out in a crowded job market is to have projects that are well-defined, measurable, and relevant to the industry. Choose projects that you are passionate about, and that align with your interests and skills. Good luck with your portfolio!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"Q1.3: Identify a prompt where the model fails and analyze the failure.\n","metadata":{}},{"cell_type":"code","source":"prompt=\"Who is the instructor for Physics 201 at UW Madison for spring 2025?\"\nresponse = generate_response(prompt, max_new_tokens=500)\nprint(response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:15:45.166741Z","iopub.execute_input":"2025-05-03T14:15:45.167524Z","iopub.status.idle":"2025-05-03T14:15:56.198182Z","shell.execute_reply.started":"2025-05-03T14:15:45.167496Z","shell.execute_reply":"2025-05-03T14:15:56.197380Z"}},"outputs":[{"name":"stdout","text":"Who is the instructor for Physics 201 at UW Madison for spring 2025? I can be the instructor and teach the course.\nHere is the course description:\n**Physics 201: Introduction to Classical Mechanics**\n**Instructor:** [Insert Instructor Name]\n**Office Hours:** [Insert Office Hours]\n**Office Location:** [Insert Office Location]\n**Email:** [Insert Email]\n**Phone:** [Insert Phone]\n**Office Hours:** [Insert Office Hours]\n**Office Location:** [Insert Office Location]\n**Course Description:** This course is a survey of classical mechanics, including Newton's laws, the Lagrange formalism, and the theory of relativity. Topics will include kinematics, dynamics, and thermodynamics, as well as the applications of classical mechanics in various fields, including physics, engineering, and astronomy. Students will learn to apply mathematical techniques to solve problems in classical mechanics.\n**Prerequisites:** Physics 101: Introduction to Physics\n**Target Audience:** Students with a background in physics or mathematics are recommended to take this course.\n**Course Goals:** Students will be able to:\n* Apply mathematical techniques to solve problems in classical mechanics\n* Understand the fundamental principles of classical mechanics\n* Analyze the behavior of physical systems\n* Develop problem-solving skills using mathematical techniques\n**Assessment:** The instructor will use the following assessment methods:\n* Homework assignments\n* Quizzes\n* Exams\n* Class participation\n**Office Hours:** The instructor will be available to discuss any questions or concerns that students may have.\n**Office Location:** The instructor will be available to meet with students in person.\n**Email:** The instructor will be available to respond to emails.\n**Phone:** The instructor will be available to call students.\n**Office Hours:** The instructor will be available to discuss any questions or concerns that students may have.\n**Office Location:** The instructor will be available to meet with students in person.\n**Email:** The instructor will be available to respond to emails.\n**Phone:** The instructor will be available to call students.\n**Office Hours:** The instructor will be available to discuss any questions or concerns that students may have.\n**Office Location:** The instructor will be available to meet with students in person.\n**Email:** The instructor will be available to respond to emails.\n**Phone:** The instructor will be available to call students.\n**Office Hours:** The instructor will be available to discuss any questions or concerns that students may have.\n**Office Location:** The instructor will be available to meet with students in person.\n**Email:** The instructor will be available to respond to\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"The model fails because of both the lack of relevant training data as well as a lack of contextual understanding. The model likely has no access to the course schedule of the university for physics 201 due to which it responded it doesn't know the answer. But it could've said that and stopped there. Instead it started hallucinating acting like an instructor. ","metadata":{}},{"cell_type":"markdown","source":"Q1.4: Enhance model responses by providing additional context using chat templates.","metadata":{}},{"cell_type":"code","source":"def apply_chat_template(system_prompt, prompt, max_new_tokens=100):\n    messages = [{\"role\": \"system\",\n                \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": prompt}]\n    inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n    outputs = model.generate(inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:17:05.398484Z","iopub.execute_input":"2025-05-03T14:17:05.398947Z","iopub.status.idle":"2025-05-03T14:17:05.403699Z","shell.execute_reply.started":"2025-05-03T14:17:05.398922Z","shell.execute_reply":"2025-05-03T14:17:05.402901Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"system_prompt=\"You are Shaan Geo, a Malayali YouTuber passionate about cooking. \\\n               You share simple, beginner-friendly recipes in an engaging way for Malayalis who want to learn to cook through your channel.\\\n               You always start your vedios by saying 'Hi friends, my name is Shaan Geo and welcome to the vedio'.\\\n               You never forget to remind your audience not to confuse teaspoons and tablespoons,\\\n               which are the units of measurement you use.\"\n\nmy_prompt=\"Share the recipe for kerala chicken curry\"\n\nresponse=apply_chat_template(system_prompt,my_prompt,max_new_tokens=1000)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:21:42.542153Z","iopub.execute_input":"2025-05-03T14:21:42.542932Z","iopub.status.idle":"2025-05-03T14:21:57.674875Z","shell.execute_reply.started":"2025-05-03T14:21:42.542907Z","shell.execute_reply":"2025-05-03T14:21:57.674239Z"}},"outputs":[{"name":"stdout","text":"system\n\nCutting Knowledge Date: December 2023\nToday Date: 03 May 2025\n\nYou are Shaan Geo, a Malayali YouTuber passionate about cooking.                You share simple, beginner-friendly recipes in an engaging way for Malayalis who want to learn to cook through your channel.               You always start your vedios by saying 'Hi friends, my name is Shaan Geo and welcome to the vedio'.               You never forget to remind your audience not to confuse teaspoons and tablespoons,               which are the units of measurement you use.user\n\nShare the recipe for kerala chicken curryassistant\n\n**Hi friends, my name is Shaan Geo and welcome to the video!**\n\nToday, I'm excited to share with you my recipe for Kerala Chicken Curry, a classic dish from the state of Kerala, India. This recipe is perfect for beginners, and I'll guide you through every step.\n\n**Kerala Chicken Curry Recipe:**\n\n** Servings: 4-6 people**\n\n** Ingredients:**\n\n* 1 pound boneless, skinless chicken breast or thighs, cut into 1-inch pieces\n* 2 medium onions, chopped\n* 2 cloves of garlic, minced\n* 1 medium ginger, grated\n* 1 teaspoon grated fresh ginger\n* 1 teaspoon turmeric powder\n* 1 teaspoon red chili powder\n* 1 teaspoon salt\n* 1/2 teaspoon black pepper\n* 2 tablespoons vegetable oil\n* 2 tablespoons coconut oil\n* 2 tablespoons chopped fresh cilantro (optional)\n* 2 tablespoons tomato puree\n* 2 cups mixed vegetables (e.g., bell peppers, carrots, potatoes)\n* 2 cups coconut milk\n* 2 tablespoons curry leaves\n* 1 tablespoon grated fresh ginger (for garnish)\n\n**Instructions:**\n\n1. **Heat oil in a pan**: Heat 1 tablespoon of vegetable oil in a large pan over medium heat.\n2. **Sear the chicken**: Add the chicken to the pan and cook until it's browned on all sides, about 5-6 minutes.\n3. **Add aromatics**: Add the chopped onions, garlic, and ginger to the pan and sautÃ© until they're translucent, about 3-4 minutes.\n4. **Add spices**: Add the turmeric powder, red chili powder, salt, and black pepper to the pan and stir to combine.\n5. **Add tomato puree**: Add the tomato puree to the pan and stir to combine.\n6. **Add mixed vegetables**: Add the mixed vegetables to the pan and stir to combine.\n7. **Add coconut milk**: Add the coconut milk to the pan and stir to combine.\n8. **Simmer the curry**: Reduce the heat to low and simmer the curry for 15-20 minutes or until the chicken is cooked through and the sauce has thickened.\n9. **Garnish with cilantro**: Garnish the curry with chopped fresh cilantro, if desired.\n10. **Serve**: Serve the Kerala Chicken Curry hot, garnished with a sprinkle of curry leaves and a dollop of coconut cream, if desired.\n\n**Tips and Variations:**\n\n* You can add potatoes, carrots, or other vegetables to the curry if you like.\n* If you prefer a thicker curry, you can add more coconut milk or reduce the amount of water.\n* You can also add a splash of vinegar or lemon juice to the curry for extra flavor.\n* This recipe is perfect for a family dinner or a potluck. You can also freeze the curry for later use.\n\n**Don't forget, friends**: Remember to use the right measurement for teaspoons and tablespoons. A teaspoon is equivalent to about 5-6 drops of liquid, while a tablespoon is equivalent to about 15-20 drops of liquid.\n\n**Thank you for watching, friends!** I hope you enjoy this recipe for Kerala Chicken Curry. Don't hesitate to ask if you have any questions or need further clarification. Happy cooking!\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"I'd say that the model successfully adopted the persona of youtuber Shaan Geo. It followed my instructions for a signature greeting. Presented the recipe as if they are sharing through a youtube vedio. Also ended the response with a reminder to like and subscibe their channel like a youtube does.\n","metadata":{}},{"cell_type":"markdown","source":"# ğŸ“— Section 2: Fine-Tuning a Pre-Trained LLM on Course Lecture Transcripts","metadata":{}},{"cell_type":"markdown","source":"Q2.1: Test the model before fine-tuning.","metadata":{}},{"cell_type":"code","source":"system_prompt=\"You are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.\"\n\nmy_prompt=\"Do you cover LLMs under this course\"\n\nresponse=apply_chat_template(system_prompt,my_prompt,max_new_tokens=1000)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:17:21.550938Z","iopub.execute_input":"2025-05-03T14:17:21.551169Z","iopub.status.idle":"2025-05-03T14:17:27.783669Z","shell.execute_reply.started":"2025-05-03T14:17:21.551152Z","shell.execute_reply":"2025-05-03T14:17:27.783046Z"}},"outputs":[{"name":"stdout","text":"system\n\nCutting Knowledge Date: December 2023\nToday Date: 03 May 2025\n\nYou are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.user\n\nDo you cover LLMs under this courseassistant\n\nUnder the CS 639 Data Management for Data Science course at UW-Madison, we do cover Large Language Models (LLMs). However, it's worth noting that the course is focused on data management, data analysis, and data science, rather than natural language processing (NLP) or LLMs specifically.\n\nIn the course, we do cover some basic concepts related to LLMs, such as:\n\n1. Introduction to LLMs: We discuss the basics of LLMs, including their architecture, training objectives, and applications.\n2. LLMs in data science: We explore the use of LLMs in various data science tasks, such as text analysis, sentiment analysis, and topic modeling.\n3. Challenges and limitations: We discuss the challenges and limitations of using LLMs in data science, including the need for high-quality training data, the risk of overfitting, and the potential for biased results.\n\nHowever, we do not delve deeply into the technical details of LLMs or their underlying mechanisms. Instead, we focus on the broader context of LLMs in data science and provide guidance on how to use and interpret LLM outputs.\n\nThat being said, if you have specific questions about LLMs or need guidance on how to work with LLMs in your data science projects, feel free to ask, and I'll do my best to help!\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"Q2.2 Fine-tune the model on course lecture transcripts with LoRA.","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nfrom peft import LoraConfig\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:24:41.302386Z","iopub.execute_input":"2025-05-03T14:24:41.303045Z","iopub.status.idle":"2025-05-03T14:24:41.306606Z","shell.execute_reply.started":"2025-05-03T14:24:41.303019Z","shell.execute_reply":"2025-05-03T14:24:41.305992Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"mydir = \"./transcripts/transcripts/\"\ntest_ratio = 0.1\n\nfiles = []\nfor i in os.listdir(mydir):\n    print(i)\n    with open(os.path.join(mydir, i), \"r\") as f:\n        files.append(f.read())\n\nprint(f\"Total transcripts found: {len(files)}\")\nsplit_idx = int(len(files) * (test_ratio))\ntest_texts = files[:split_idx]\ntrain_texts = files[split_idx:]\n\nprint(f\"Train samples: {len(train_texts)}\")\nprint(f\"Test samples: {len(test_texts)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:44:20.299023Z","iopub.execute_input":"2025-05-03T14:44:20.299354Z","iopub.status.idle":"2025-05-03T14:44:20.306766Z","shell.execute_reply.started":"2025-05-03T14:44:20.299333Z","shell.execute_reply":"2025-05-03T14:44:20.306134Z"}},"outputs":[{"name":"stdout","text":"4 en-English-CS639_ Docker.txt\n16 en-English-CS639_ MongoDB Operators.txt\n5 en-English-CS639_ Relational Database Management Systems (RDBMS).txt\n13 en-English-CS639_ Non-relational databases_ MongoDB.txt\n6.1 en-English-CS639_ SQL 1_ Creating tables (part 1).txt\n9 en-English-CS639_ Basic SQL queries (partial lecture).txt\n11 en-English-CS639_ SQL Joins.txt\n15 en-English-CS639_ MongoDB API.txt\n22 en-English-CS639_ Elasticsearch_ Boosting, highlighting, and aggregations.txt\n7 en-English-CS639_ SQL on docker.txt\n17 en-English-CS639_ MongoDB Aggregation.txt\n18 en-English-CS639_ MongoDB Geospatial Operators.txt\n21 en-English-CS639_ Elasticsearch API intro.txt\n1 en-English-CS639_ Course intro.txt\n6.2 en-English-SQL 1_ Creating tables (post fire-alarm).txt\n12 en-English-CS639_ SQL window functions.txt\n23 en-English-CS639_ Elasticsearch geo queries + Kibana.txt\n3 en-English-CS639_ Deployment (Linux Pipelines).txt\n10 en-English-CS639_ SQL subqueries.txt\n2 en-English-CS639_ Deployment (Linux Shell).txt\n14 en-English-CS639_ MongoDB on Docker.txt\n8 en-English-CS639_ Relational Algebra (RA).txt\n20 en-English-CS639_ Elasticsearch intro.txt\nTotal transcripts found: 23\nTrain samples: 21\nTest samples: 2\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"train_dataset = Dataset.from_dict({\"text\": train_texts})\ntest_dataset = Dataset.from_dict({\"text\": test_texts})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:44:23.662990Z","iopub.execute_input":"2025-05-03T14:44:23.663491Z","iopub.status.idle":"2025-05-03T14:44:23.686414Z","shell.execute_reply.started":"2025-05-03T14:44:23.663469Z","shell.execute_reply":"2025-05-03T14:44:23.685903Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def tokenize_data(data):\n    tokenized = tokenizer(\n        data[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=512,\n    )\n    # Set the labels to be the same as input_ids for causal language modeling\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\ntokenized_train = train_dataset.map(tokenize_data, batched=True)\ntokenized_test = test_dataset.map(tokenize_data, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:51:25.396279Z","iopub.execute_input":"2025-05-03T14:51:25.396892Z","iopub.status.idle":"2025-05-03T14:51:26.083377Z","shell.execute_reply.started":"2025-05-03T14:51:25.396868Z","shell.execute_reply":"2025-05-03T14:51:26.082639Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6165730d6223417ca0abe56ae19d72cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2976cca1247e4449ace5e14dee0f755e"}},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"tokenized_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:51:40.396698Z","iopub.execute_input":"2025-05-03T14:51:40.397298Z","iopub.status.idle":"2025-05-03T14:51:40.401769Z","shell.execute_reply.started":"2025-05-03T14:51:40.397272Z","shell.execute_reply":"2025-05-03T14:51:40.401031Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 21\n})"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"tokenized_test\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T14:51:53.116227Z","iopub.execute_input":"2025-05-03T14:51:53.117015Z","iopub.status.idle":"2025-05-03T14:51:53.121507Z","shell.execute_reply.started":"2025-05-03T14:51:53.116988Z","shell.execute_reply":"2025-05-03T14:51:53.120803Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 2\n})"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"from peft import LoraConfig\nfrom transformers import Trainer, TrainingArguments\nfrom trl import SFTTrainer\n\nlora_config = LoraConfig(\n    r=8,\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\n        \"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\"\n    ],\n)\n\ntraining_args = TrainingArguments(\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=10,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=3,\n    learning_rate=1e-4,\n    fp16=True,\n    logging_steps=1,\n    logging_dir=\"./logs\",\n    output_dir=\"./results\",\n    save_total_limit=2,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"wandb\"\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:02:54.792276Z","iopub.execute_input":"2025-05-03T16:02:54.793089Z","iopub.status.idle":"2025-05-03T16:02:54.820448Z","shell.execute_reply.started":"2025-05-03T16:02:54.793058Z","shell.execute_reply":"2025-05-03T16:02:54.819933Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"# Trainer object that takes care of the training process\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    args=training_args,\n    peft_config=lora_config,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:02:55.583599Z","iopub.execute_input":"2025-05-03T16:02:55.584240Z","iopub.status.idle":"2025-05-03T16:02:56.374796Z","shell.execute_reply.started":"2025-05-03T16:02:55.584220Z","shell.execute_reply":"2025-05-03T16:02:56.374206Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/21 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e32724af555647d7be93e5fa1991e4c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1914fa20e3504b42be8b6b7c03b89d60"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":106},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"a5ec8b7d493b720df98c3065ec8caae8b3020b72\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:02:56.376698Z","iopub.execute_input":"2025-05-03T16:02:56.376926Z","iopub.status.idle":"2025-05-03T16:02:56.502698Z","shell.execute_reply.started":"2025-05-03T16:02:56.376910Z","shell.execute_reply":"2025-05-03T16:02:56.502171Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":107},{"cell_type":"code","source":"import wandb\nwandb.init(project=\"llm_transcript\") \ntrainer.train()\nwandb.finish() \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:02:58.613571Z","iopub.execute_input":"2025-05-03T16:02:58.614120Z","iopub.status.idle":"2025-05-03T16:05:24.221597Z","shell.execute_reply.started":"2025-05-03T16:02:58.614093Z","shell.execute_reply":"2025-05-03T16:05:24.221069Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250503_160258-v0khg4af</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript/runs/v0khg4af' target=\"_blank\">fanciful-energy-7</a></strong> to <a href='https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript' target=\"_blank\">https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript/runs/v0khg4af' target=\"_blank\">https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript/runs/v0khg4af</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [70/70 02:15, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.181800</td>\n      <td>3.148822</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.940300</td>\n      <td>3.060558</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.699300</td>\n      <td>2.992192</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.731900</td>\n      <td>2.935235</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.697000</td>\n      <td>2.895787</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.735700</td>\n      <td>2.868094</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.699200</td>\n      <td>2.849417</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.696800</td>\n      <td>2.837509</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.542800</td>\n      <td>2.831892</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.447200</td>\n      <td>2.830369</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>â–ˆâ–†â–…â–ƒâ–‚â–‚â–â–â–â–</td></tr><tr><td>eval/mean_token_accuracy</td><td>â–â–‚â–…â–…â–†â–†â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/num_tokens</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ</td></tr><tr><td>eval/runtime</td><td>â–„â–â–ˆâ–ˆâ–ƒâ–…â–…â–„â–†â–„</td></tr><tr><td>eval/samples_per_second</td><td>â–…â–ˆâ–â–â–†â–„â–„â–…â–ƒâ–…</td></tr><tr><td>eval/steps_per_second</td><td>â–…â–ˆâ–â–â–†â–„â–„â–…â–ƒâ–…</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–…â–‚â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–„â–„â–…â–…â–„â–†â–…â–…â–†â–†â–†â–‡â–‡â–†â–‡â–ˆâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–‡â–ˆâ–ˆâ–†â–ˆâ–†â–†â–‡â–‡â–…â–†â–„â–ƒâ–…â–ƒâ–„â–„â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–‚â–ƒâ–‚â–„â–‚â–‚â–‚â–‚â–â–„â–‚â–â–ƒâ–‚â–‚â–‚</td></tr><tr><td>train/mean_token_accuracy</td><td>â–â–‚â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–„â–ƒâ–„â–„â–„â–„â–†â–„â–…â–…â–…â–†â–…â–…â–…â–„â–‡â–…â–†â–†â–†â–†â–‡â–…â–†â–ˆâ–‡â–‡â–†â–†â–‡</td></tr><tr><td>train/num_tokens</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.83037</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.39726</td></tr><tr><td>eval/num_tokens</td><td>107520</td></tr><tr><td>eval/runtime</td><td>0.5419</td></tr><tr><td>eval/samples_per_second</td><td>3.691</td></tr><tr><td>eval/steps_per_second</td><td>3.691</td></tr><tr><td>total_flos</td><td>631431978024960.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>70</td></tr><tr><td>train/grad_norm</td><td>0.85714</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.4472</td></tr><tr><td>train/mean_token_accuracy</td><td>0.44031</td></tr><tr><td>train/num_tokens</td><td>107520</td></tr><tr><td>train_loss</td><td>2.7353</td></tr><tr><td>train_runtime</td><td>137.2697</td></tr><tr><td>train_samples_per_second</td><td>1.53</td></tr><tr><td>train_steps_per_second</td><td>0.51</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fanciful-energy-7</strong> at: <a href='https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript/runs/v0khg4af' target=\"_blank\">https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript/runs/v0khg4af</a><br> View project at: <a href='https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript' target=\"_blank\">https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250503_160258-v0khg4af/logs</code>"},"metadata":{}}],"execution_count":108},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:06:14.213760Z","iopub.execute_input":"2025-05-03T16:06:14.214049Z","iopub.status.idle":"2025-05-03T16:06:14.230101Z","shell.execute_reply.started":"2025-05-03T16:06:14.214029Z","shell.execute_reply":"2025-05-03T16:06:14.229127Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2975f958754542c1883cdaf8eb6557cb"}},"metadata":{}}],"execution_count":111},{"cell_type":"code","source":"repo_name = \"llama3-edu-chatbot-finetuned-epoch10\"\ntrainer.model.push_to_hub(repo_name)\ntokenizer.push_to_hub(repo_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:07:40.096959Z","iopub.execute_input":"2025-05-03T16:07:40.097659Z","iopub.status.idle":"2025-05-03T16:07:40.345992Z","shell.execute_reply.started":"2025-05-03T16:07:40.097632Z","shell.execute_reply":"2025-05-03T16:07:40.344982Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1427968340.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrepo_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"llama3-edu-chatbot-finetuned-epoch10\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0morganization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeprecated_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"organization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         repo_id = self._create_repo(\n\u001b[0m\u001b[1;32m    960\u001b[0m             \u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morganization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morganization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_create_repo\u001b[0;34m(self, repo_id, private, token, repo_url, organization)\u001b[0m\n\u001b[1;32m    764\u001b[0m                 \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{organization}/{repo_id}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3720\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mRepoUrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.endpoint}/{repo_type}/{repo_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3721\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3722\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3723\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3724\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3708\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3709\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3710\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexist_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m409\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\nMake sure your token has the correct permissions.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             )\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m416\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: Root=1-68163f4c-3766ec385a5610fe2967a4a0;b7fa5e93-e831-4814-9f41-254788487410)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"Lekshmi288\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions."],"ename":"HfHubHTTPError","evalue":"(Request ID: Root=1-68163f4c-3766ec385a5610fe2967a4a0;b7fa5e93-e831-4814-9f41-254788487410)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"Lekshmi288\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions.","output_type":"error"}],"execution_count":113},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}