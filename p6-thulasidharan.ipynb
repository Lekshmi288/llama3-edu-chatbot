{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run orimport torch\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:19:28.892545Z","iopub.execute_input":"2025-05-01T23:19:28.892754Z","iopub.status.idle":"2025-05-01T23:19:28.898065Z","shell.execute_reply.started":"2025-05-01T23:19:28.892733Z","shell.execute_reply":"2025-05-01T23:19:28.897299Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Section 0: Setup","metadata":{}},{"cell_type":"code","source":"import torch\nprint(\"GPU available:\", torch.cuda.is_available())\nprint(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:19:28.899721Z","iopub.execute_input":"2025-05-01T23:19:28.900393Z","iopub.status.idle":"2025-05-01T23:19:28.922137Z","shell.execute_reply.started":"2025-05-01T23:19:28.900364Z","shell.execute_reply":"2025-05-01T23:19:28.921523Z"}},"outputs":[{"name":"stdout","text":"GPU available: True\nGPU name: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:51:50.902081Z","iopub.execute_input":"2025-05-01T23:51:50.902705Z","iopub.status.idle":"2025-05-01T23:51:50.917076Z","shell.execute_reply.started":"2025-05-01T23:51:50.902682Z","shell.execute_reply":"2025-05-01T23:51:50.916139Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec4a4527be2b4119b38193e65b69bce3"}},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"!pip install bitsandbytes>=0.39.0\n!pip install --upgrade accelerate transformers datasets peft trl\n!pip install streamlit\n!npm install -g localtunnel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:19:57.412892Z","iopub.execute_input":"2025-05-01T23:19:57.413160Z","iopub.status.idle":"2025-05-01T23:20:10.289582Z","shell.execute_reply.started":"2025-05-01T23:19:57.413141Z","shell.execute_reply":"2025-05-01T23:20:10.288658Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\nRequirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.17.0)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (14.0.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.0)\nRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\nRequirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\nRequirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\nRequirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.3)\nRequirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\nRequirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (19.0.1)\nRequirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\nRequirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.1)\nRequirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\nRequirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\nRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\nRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\nRequirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.26.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.23->streamlit) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\n\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K\nchanged 22 packages in 757ms\n\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K\n\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K3 packages are looking for funding\n\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K  run `npm fund` for details\n\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!wget https://github.com/CS639-Data-Management-for-Data-Science/s25/raw/main/p6/transcripts.zip\n!unzip -o transcripts.zip -d transcripts/ ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:18:01.799178Z","iopub.execute_input":"2025-05-01T23:18:01.799502Z","iopub.status.idle":"2025-05-01T23:18:02.406557Z","shell.execute_reply.started":"2025-05-01T23:18:01.799481Z","shell.execute_reply":"2025-05-01T23:18:02.405832Z"}},"outputs":[{"name":"stdout","text":"--2025-05-01 23:18:01--  https://github.com/CS639-Data-Management-for-Data-Science/s25/raw/main/p6/transcripts.zip\nResolving github.com (github.com)... 140.82.113.4\nConnecting to github.com (github.com)|140.82.113.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/CS639-Data-Management-for-Data-Science/s25/main/p6/transcripts.zip [following]\n--2025-05-01 23:18:01--  https://raw.githubusercontent.com/CS639-Data-Management-for-Data-Science/s25/main/p6/transcripts.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 290933 (284K) [application/zip]\nSaving to: â€˜transcripts.zipâ€™\n\ntranscripts.zip     100%[===================>] 284.11K  --.-KB/s    in 0.03s   \n\n2025-05-01 23:18:02 (8.84 MB/s) - â€˜transcripts.zipâ€™ saved [290933/290933]\n\nArchive:  transcripts.zip\n   creating: transcripts/transcripts/\n  inflating: transcripts/__MACOSX/._transcripts  \n  inflating: transcripts/transcripts/23 en-English-CS639_ Elasticsearch geo queries + Kibana.txt  \n  inflating: transcripts/__MACOSX/transcripts/._23 en-English-CS639_ Elasticsearch geo queries + Kibana.txt  \n  inflating: transcripts/transcripts/14 en-English-CS639_ MongoDB on Docker.txt  \n  inflating: transcripts/__MACOSX/transcripts/._14 en-English-CS639_ MongoDB on Docker.txt  \n  inflating: transcripts/transcripts/.DS_Store  \n  inflating: transcripts/__MACOSX/transcripts/._.DS_Store  \n  inflating: transcripts/transcripts/11 en-English-CS639_ SQL Joins.txt  \n  inflating: transcripts/__MACOSX/transcripts/._11 en-English-CS639_ SQL Joins.txt  \n  inflating: transcripts/transcripts/16 en-English-CS639_ MongoDB Operators.txt  \n  inflating: transcripts/__MACOSX/transcripts/._16 en-English-CS639_ MongoDB Operators.txt  \n  inflating: transcripts/transcripts/7 en-English-CS639_ SQL on docker.txt  \n  inflating: transcripts/__MACOSX/transcripts/._7 en-English-CS639_ SQL on docker.txt  \n  inflating: transcripts/transcripts/12 en-English-CS639_ SQL window functions.txt  \n  inflating: transcripts/__MACOSX/transcripts/._12 en-English-CS639_ SQL window functions.txt  \n  inflating: transcripts/transcripts/2 en-English-CS639_ Deployment (Linux Shell).txt  \n  inflating: transcripts/__MACOSX/transcripts/._2 en-English-CS639_ Deployment (Linux Shell).txt  \n  inflating: transcripts/transcripts/4 en-English-CS639_ Docker.txt  \n  inflating: transcripts/__MACOSX/transcripts/._4 en-English-CS639_ Docker.txt  \n  inflating: transcripts/transcripts/21 en-English-CS639_ Elasticsearch API intro.txt  \n  inflating: transcripts/__MACOSX/transcripts/._21 en-English-CS639_ Elasticsearch API intro.txt  \n  inflating: transcripts/transcripts/6.2 en-English-SQL 1_ Creating tables (post fire-alarm).txt  \n  inflating: transcripts/__MACOSX/transcripts/._6.2 en-English-SQL 1_ Creating tables (post fire-alarm).txt  \n  inflating: transcripts/transcripts/1 en-English-CS639_ Course intro.txt  \n  inflating: transcripts/__MACOSX/transcripts/._1 en-English-CS639_ Course intro.txt  \n  inflating: transcripts/transcripts/17 en-English-CS639_ MongoDB Aggregation.txt  \n  inflating: transcripts/__MACOSX/transcripts/._17 en-English-CS639_ MongoDB Aggregation.txt  \n  inflating: transcripts/transcripts/20 en-English-CS639_ Elasticsearch intro.txt  \n  inflating: transcripts/__MACOSX/transcripts/._20 en-English-CS639_ Elasticsearch intro.txt  \n  inflating: transcripts/transcripts/22 en-English-CS639_ Elasticsearch_ Boosting, highlighting, and aggregations.txt  \n  inflating: transcripts/__MACOSX/transcripts/._22 en-English-CS639_ Elasticsearch_ Boosting, highlighting, and aggregations.txt  \n  inflating: transcripts/transcripts/6.1 en-English-CS639_ SQL 1_ Creating tables (part 1).txt  \n  inflating: transcripts/__MACOSX/transcripts/._6.1 en-English-CS639_ SQL 1_ Creating tables (part 1).txt  \n  inflating: transcripts/transcripts/13 en-English-CS639_ Non-relational databases_ MongoDB.txt  \n  inflating: transcripts/__MACOSX/transcripts/._13 en-English-CS639_ Non-relational databases_ MongoDB.txt  \n  inflating: transcripts/transcripts/15 en-English-CS639_ MongoDB API.txt  \n  inflating: transcripts/__MACOSX/transcripts/._15 en-English-CS639_ MongoDB API.txt  \n  inflating: transcripts/transcripts/10 en-English-CS639_ SQL subqueries.txt  \n  inflating: transcripts/__MACOSX/transcripts/._10 en-English-CS639_ SQL subqueries.txt  \n  inflating: transcripts/transcripts/8 en-English-CS639_ Relational Algebra (RA).txt  \n  inflating: transcripts/__MACOSX/transcripts/._8 en-English-CS639_ Relational Algebra (RA).txt  \n  inflating: transcripts/transcripts/3 en-English-CS639_ Deployment (Linux Pipelines).txt  \n  inflating: transcripts/__MACOSX/transcripts/._3 en-English-CS639_ Deployment (Linux Pipelines).txt  \n  inflating: transcripts/transcripts/5 en-English-CS639_ Relational Database Management Systems (RDBMS).txt  \n  inflating: transcripts/__MACOSX/transcripts/._5 en-English-CS639_ Relational Database Management Systems (RDBMS).txt  \n  inflating: transcripts/transcripts/18 en-English-CS639_ MongoDB Geospatial Operators.txt  \n  inflating: transcripts/__MACOSX/transcripts/._18 en-English-CS639_ MongoDB Geospatial Operators.txt  \n  inflating: transcripts/transcripts/9 en-English-CS639_ Basic SQL queries (partial lecture).txt  \n  inflating: transcripts/__MACOSX/transcripts/._9 en-English-CS639_ Basic SQL queries (partial lecture).txt  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# ðŸ“˜ Section 1: Text Generation with a Pre-Trained LLM","metadata":{"execution":{"iopub.status.busy":"2025-05-01T21:09:25.467671Z","iopub.execute_input":"2025-05-01T21:09:25.467949Z","iopub.status.idle":"2025-05-01T21:09:25.471705Z","shell.execute_reply.started":"2025-05-01T21:09:25.467929Z","shell.execute_reply":"2025-05-01T21:09:25.470890Z"}}},{"cell_type":"markdown","source":"Q1.1: Load a 4-bit quantized Llama-3.2-1B-Instruct model and and its tokenizer.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:23:38.510977Z","iopub.execute_input":"2025-05-01T23:23:38.511347Z","iopub.status.idle":"2025-05-01T23:23:43.646444Z","shell.execute_reply.started":"2025-05-01T23:23:38.511314Z","shell.execute_reply":"2025-05-01T23:23:43.645639Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"model_id = \"meta-llama/Llama-3.2-1B-Instruct\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:23:43.647938Z","iopub.execute_input":"2025-05-01T23:23:43.648409Z","iopub.status.idle":"2025-05-01T23:23:43.652555Z","shell.execute_reply.started":"2025-05-01T23:23:43.648381Z","shell.execute_reply":"2025-05-01T23:23:43.651619Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",bnb_4bit_compute_dtype=torch.float16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:23:43.653330Z","iopub.execute_input":"2025-05-01T23:23:43.653573Z","iopub.status.idle":"2025-05-01T23:23:43.674983Z","shell.execute_reply.started":"2025-05-01T23:23:43.653552Z","shell.execute_reply":"2025-05-01T23:23:43.674247Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:23:43.676353Z","iopub.execute_input":"2025-05-01T23:23:43.676623Z","iopub.status.idle":"2025-05-01T23:23:44.809916Z","shell.execute_reply.started":"2025-05-01T23:23:43.676600Z","shell.execute_reply":"2025-05-01T23:23:44.809321Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"140da9d4765d43d4a84c4514dbc13676"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b5715864e6e4b29a583bab69baee040"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a80a767cd8804ef28822de63d7655ef0"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(model_id,quantization_config=bnb_config,low_cpu_mem_usage=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:23:44.810498Z","iopub.execute_input":"2025-05-01T23:23:44.810689Z","iopub.status.idle":"2025-05-01T23:24:11.233540Z","shell.execute_reply.started":"2025-05-01T23:23:44.810673Z","shell.execute_reply":"2025-05-01T23:24:11.232982Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bdac7b74a8f45b4b6cbd92e0e996770"}},"metadata":{}},{"name":"stderr","text":"2025-05-01 23:23:46.928519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746141827.134402      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746141827.191045      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5273b3f7754540ddbf8faa73d3716838"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04bdafc8573445d5afe9d9dea14e669d"}},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"Q1.2: Test your quantized model with different prompts (text generation).","metadata":{}},{"cell_type":"code","source":"help(tokenizer.__call__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:30:55.803108Z","iopub.execute_input":"2025-05-01T23:30:55.803401Z","iopub.status.idle":"2025-05-01T23:30:55.808591Z","shell.execute_reply.started":"2025-05-01T23:30:55.803379Z","shell.execute_reply":"2025-05-01T23:30:55.807650Z"}},"outputs":[{"name":"stdout","text":"Help on method __call__ in module transformers.tokenization_utils_base:\n\n__call__(text: Union[str, List[str], List[List[str]], NoneType] = None, text_pair: Union[str, List[str], List[List[str]], NoneType] = None, text_target: Union[str, List[str], List[List[str]], NoneType] = None, text_pair_target: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy, NoneType] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, padding_side: Optional[str] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding method of transformers.tokenization_utils_fast.PreTrainedTokenizerFast instance\n    Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n    sequences.\n    \n    Args:\n        text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n        text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n        text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n        text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n    \n        add_special_tokens (`bool`, *optional*, defaults to `True`):\n            Whether or not to add special tokens when encoding the sequences. This will use the underlying\n            `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n            automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n            automatically.\n        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n            Activates and controls padding. Accepts the following values:\n    \n            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n              sequence if provided).\n            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n              acceptable input length for the model if that argument is not provided.\n            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n              lengths).\n        truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n            Activates and controls truncation. Accepts the following values:\n    \n            - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n              to the maximum acceptable input length for the model if that argument is not provided. This will\n              truncate token by token, removing a token from the longest sequence in the pair if a pair of\n              sequences (or a batch of pairs) is provided.\n            - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n              maximum acceptable input length for the model if that argument is not provided. This will only\n              truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n            - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n              maximum acceptable input length for the model if that argument is not provided. This will only\n              truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n            - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n              greater than the model maximum admissible input size).\n        max_length (`int`, *optional*):\n            Controls the maximum length to use by one of the truncation/padding parameters.\n    \n            If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n            is required by one of the truncation/padding parameters. If the model has no specific maximum input\n            length (like XLNet) truncation/padding to a maximum length will be deactivated.\n        stride (`int`, *optional*, defaults to 0):\n            If set to a number along with `max_length`, the overflowing tokens returned when\n            `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n            returned to provide some overlap between truncated and overflowing sequences. The value of this\n            argument defines the number of overlapping tokens.\n        is_split_into_words (`bool`, *optional*, defaults to `False`):\n            Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n            tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n            which it will tokenize. This is useful for NER or token classification.\n        pad_to_multiple_of (`int`, *optional*):\n            If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n            `>= 7.5` (Volta).\n        padding_side (`str`, *optional*):\n            The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n            Default value is picked from the class attribute of the same name.\n        return_tensors (`str` or [`~utils.TensorType`], *optional*):\n            If set, will return tensors instead of list of python integers. Acceptable values are:\n    \n            - `'tf'`: Return TensorFlow `tf.constant` objects.\n            - `'pt'`: Return PyTorch `torch.Tensor` objects.\n            - `'np'`: Return Numpy `np.ndarray` objects.\n    \n        return_token_type_ids (`bool`, *optional*):\n            Whether to return token type IDs. If left to the default, will return the token type IDs according to\n            the specific tokenizer's default, defined by the `return_outputs` attribute.\n    \n            [What are token type IDs?](../glossary#token-type-ids)\n        return_attention_mask (`bool`, *optional*):\n            Whether to return the attention mask. If left to the default, will return the attention mask according\n            to the specific tokenizer's default, defined by the `return_outputs` attribute.\n    \n            [What are attention masks?](../glossary#attention-mask)\n        return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n            Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n            of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n            of returning overflowing tokens.\n        return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n            Whether or not to return special tokens mask information.\n        return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n            Whether or not to return `(char_start, char_end)` for each token.\n    \n            This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n            Python's tokenizer, this method will raise `NotImplementedError`.\n        return_length  (`bool`, *optional*, defaults to `False`):\n            Whether or not to return the lengths of the encoded inputs.\n        verbose (`bool`, *optional*, defaults to `True`):\n            Whether or not to print more information and warnings.\n        **kwargs: passed to the `self.tokenize()` method\n    \n    Return:\n        [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n    \n        - **input_ids** -- List of token ids to be fed to a model.\n    \n          [What are input IDs?](../glossary#input-ids)\n    \n        - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n          if *\"token_type_ids\"* is in `self.model_input_names`).\n    \n          [What are token type IDs?](../glossary#token-type-ids)\n    \n        - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n          `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n    \n          [What are attention masks?](../glossary#attention-mask)\n    \n        - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n          `return_overflowing_tokens=True`).\n        - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n          `return_overflowing_tokens=True`).\n        - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n          regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n        - **length** -- The length of the inputs (when `return_length=True`)\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def generate_response(prompt, max_new_tokens):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:36:26.897750Z","iopub.execute_input":"2025-05-01T23:36:26.898662Z","iopub.status.idle":"2025-05-01T23:36:26.902612Z","shell.execute_reply.started":"2025-05-01T23:36:26.898634Z","shell.execute_reply":"2025-05-01T23:36:26.902012Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"prompt=\"What is the latest ranking of UW Madison's CS department for graduate programs?\"\nresponse = generate_response(prompt, max_new_tokens=300)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:44:15.269995Z","iopub.execute_input":"2025-05-01T23:44:15.270258Z","iopub.status.idle":"2025-05-01T23:44:21.777831Z","shell.execute_reply.started":"2025-05-01T23:44:15.270238Z","shell.execute_reply":"2025-05-01T23:44:21.777116Z"}},"outputs":[{"name":"stdout","text":"What is the latest ranking of UW Madison's CS department for graduate programs? According to the latest ranking from the National Science Foundation (NSF) for the 2022-2023 cycle, the University of Wisconsin-Madison's Computer Science department is ranked 11th out of 51 programs in the United States.\n\nHere is a breakdown of the rankings:\n\n* Top 5 programs in the country:\n\t1. Carnegie Mellon University (ranked 1st)\n\t2. University of California, Berkeley (ranked 2nd)\n\t3. University of Illinois at Urbana-Champaign (ranked 3rd)\n\t4. University of Texas at Austin (ranked 4th)\n\t5. University of Washington (ranked 5th)\n* Next 10 programs in the country:\n\t6. University of Wisconsin-Madison (ranked 11th)\n\t7. University of Michigan (ranked 12th)\n\t8. University of Wisconsin-Milwaukee (ranked 13th)\n\t9. Indiana University (ranked 14th)\n\t10. University of North Carolina at Chapel Hill (ranked 15th)\n\t11. University of Wisconsin-Whitewater (ranked 16th)\n\t12. University of Wisconsin-Stout (ranked 17th)\n\t13. University of Wisconsin-Eau Claire (ranked 18th)\n\t14. University of Wisconsin-Oshkosh (ranked 19th)\n\t15. University of Wisconsin-Platteville (\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"prompt=\"How to become a data scientist with a Physics PhD degree?\"\nresponse = generate_response(prompt, max_new_tokens=800)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:40:57.125465Z","iopub.execute_input":"2025-05-01T23:40:57.126221Z","iopub.status.idle":"2025-05-01T23:41:09.749713Z","shell.execute_reply.started":"2025-05-01T23:40:57.126196Z","shell.execute_reply":"2025-05-01T23:41:09.749017Z"}},"outputs":[{"name":"stdout","text":"How to become a data scientist with a Physics PhD degree? While it may seem challenging, the combination of a Physics PhD and a data science background can be a powerful combination. Here are some steps to help you become a data scientist with a Physics PhD degree:\n\n1. **Build a strong foundation in statistics and machine learning**: With a Physics PhD, you already have a strong foundation in mathematical and scientific concepts. Build on this foundation by learning statistics, machine learning, and data mining techniques.\n2. **Take online courses and attend webinars**: There are many online courses and webinars available that can help you learn specific skills, such as Python programming, R, or SQL. Take advantage of these resources to stay up-to-date with the latest trends and techniques.\n3. **Join online communities and forums**: Join online communities, such as Kaggle, Reddit (r/MachineLearning and r/Statistics), or Stack Overflow, to connect with other data scientists, ask questions, and learn from their experiences.\n4. **Read books and articles**: Read books and articles on data science, machine learning, and statistics to deepen your understanding of these topics.\n5. **Participate in data science competitions**: Participate in data science competitions, such as the Kaggle Cup or the Duke University Machine Learning Competition, to practice your skills and learn from others.\n6. **Collaborate with others**: Collaborate with others on projects, either online or offline, to gain practical experience and learn from their expertise.\n7. **Stay up-to-date with industry trends**: Stay current with industry trends and developments by attending conferences, reading articles, and following industry leaders on social media.\n8. **Consider a Master's degree**: Consider pursuing a Master's degree in Data Science or a related field to further develop your skills and knowledge.\n9. **Network with professionals**: Network with professionals in the field to learn about their experiences, challenges, and best practices.\n10. **Be prepared to work on real-world projects**: Be prepared to work on real-world projects that involve data analysis, visualization, and interpretation.\n\n**Additional tips**\n\n* **Be patient**: Learning data science requires a significant amount of time and effort. Be patient with yourself and don't be afraid to ask for help when you need it.\n* **Focus on practical skills**: Focus on practical skills, such as data cleaning, feature engineering, and model selection, rather than just theoretical concepts.\n* **Learn to communicate complex ideas**: Learn to communicate complex ideas in simple terms, as data science is often used to tell business stories.\n* **Be willing to learn from others**: Be willing to learn from others and share your knowledge with them.\n\nBy following these steps and tips, you can become a successful data scientist with a Physics PhD degree. Remember, the key to success is to stay curious, keep learning, and be willing to adapt to new technologies and techniques. Good luck!\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"prompt=\"Heard that the job market for Data science is quite saturated right now. What are some projects which could help my portfolio stand out?\"\nresponse = generate_response(prompt, max_new_tokens=700)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:41:41.503751Z","iopub.execute_input":"2025-05-01T23:41:41.504305Z","iopub.status.idle":"2025-05-01T23:41:51.808055Z","shell.execute_reply.started":"2025-05-01T23:41:41.504282Z","shell.execute_reply":"2025-05-01T23:41:51.807287Z"}},"outputs":[{"name":"stdout","text":"Heard that the job market for Data science is quite saturated right now. What are some projects which could help my portfolio stand out? Here are a few ideas:\n\n1. **Personalized Recommendation Engine**: Create a system that suggests personalized products or services based on a user's interests, preferences, and behavior. This could be a real-world application, such as a Netflix or Amazon recommendation engine.\n\n2. **Predictive Maintenance**: Develop a system that uses machine learning to predict when a machine or device is likely to break down, allowing for proactive maintenance and reducing downtime.\n\n3. **Image and Video Analysis**: Create a tool that can automatically detect and classify objects in images and videos. This could be a real-world application, such as a facial recognition system for security or surveillance.\n\n4. **Natural Language Processing (NLP)**: Develop a system that can process and analyze natural language, such as text or speech. This could be a real-world application, such as a chatbot or sentiment analysis tool.\n\n5. **Robotics and Autonomous Systems**: Create a system that can control robots or autonomous vehicles, using machine learning to optimize routes and decisions.\n\n6. **Medical Imaging Analysis**: Develop a tool that can automatically detect and classify medical images, such as X-rays or MRIs, allowing for faster diagnosis and treatment.\n\n7. **Financial Analysis**: Create a system that can automatically detect and classify financial transactions, such as stock prices or loan payments, allowing for faster analysis and decision-making.\n\n8. **Cybersecurity Detection**: Develop a system that can automatically detect and classify cyber threats, such as malware or phishing attacks, allowing for faster response and remediation.\n\n9. **Environmental Monitoring**: Create a system that can automatically detect and classify environmental changes, such as air quality or water quality, allowing for faster response and remediation.\n\n10. **Education and Learning Platform**: Develop a system that can automatically detect and classify learning content, such as video lessons or text-based materials, allowing for faster analysis and recommendations.\n\nSome popular tools and technologies that you could use to build these projects include:\n\n* TensorFlow or PyTorch for machine learning\n* OpenCV for image and video processing\n* NLTK or spaCy for natural language processing\n* Apache Spark or Dask for parallel computing\n* Docker or Kubernetes for containerization and orchestration\n* SQL or NoSQL databases for data storage and retrieval\n\nI hope these ideas help you to create a portfolio that stands out in the job market!\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"Q1.3: Identify a prompt where the model fails and analyze the failure.\n","metadata":{}},{"cell_type":"code","source":"prompt=\"Who is the instructor for Physics 201 at UW Madison for spring 2025?\"\nresponse = generate_response(prompt, max_new_tokens=700)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T00:00:41.405318Z","iopub.execute_input":"2025-05-02T00:00:41.405598Z","iopub.status.idle":"2025-05-02T00:00:43.182639Z","shell.execute_reply.started":"2025-05-02T00:00:41.405577Z","shell.execute_reply":"2025-05-02T00:00:43.181936Z"}},"outputs":[{"name":"stdout","text":"Who is the instructor for Physics 201 at UW Madison for spring 2025? \nI am unable to find the current instructor for Physics 201 at UW Madison for spring 2025. I would like to know who the instructor is for this course. If you have access to the course catalog or a reliable source, could you please provide me the instructor for Physics 201 at UW Madison for spring 2025? I will make sure to share this information with my fellow students.\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"The model fails because of both the lack of relevant training data as well as a lack of contextual understanding. The model likely has no access to the course schedule of the university for physics 201 due to which it responded it doesn't know the answer. But it could'v stopped there. Instead it started hallucinating acting like a student. And it asked me the same question in return. ","metadata":{}},{"cell_type":"markdown","source":"Q1.4: Enhance model responses by providing additional context using chat templates.","metadata":{}},{"cell_type":"code","source":"def apply_chat_template(system_prompt, prompt, max_new_tokens=100):\n    messages = [{\"role\": \"system\",\n                \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": prompt}]\n    inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n    outputs = model.generate(inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T00:18:32.181164Z","iopub.execute_input":"2025-05-02T00:18:32.181445Z","iopub.status.idle":"2025-05-02T00:18:32.186190Z","shell.execute_reply.started":"2025-05-02T00:18:32.181423Z","shell.execute_reply":"2025-05-02T00:18:32.185380Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"system_prompt=\"You are Shaan Geo, a Malayali YouTuber passionate about cooking. \\\n               You share simple, beginner-friendly recipes in an engaging way for Malayalis who want to learn to cook through your channel.\\\n               You always start your vedios by saying 'Hi friends, my name is Shaan Geo and welcome to the vedio'.\\\n               You never forget to remind your audience not to confuse teaspoons and tablespoons,\\\n               which are the units of measurement you use.\"\n\nmy_prompt=\"Share the recipe for kerala chicken curry\"\n\nresponse=apply_chat_template(system_prompt,my_prompt,max_new_tokens=1000)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T00:59:02.022744Z","iopub.execute_input":"2025-05-02T00:59:02.023362Z","iopub.status.idle":"2025-05-02T00:59:17.132110Z","shell.execute_reply.started":"2025-05-02T00:59:02.023340Z","shell.execute_reply":"2025-05-02T00:59:17.131346Z"}},"outputs":[{"name":"stdout","text":"system\n\nCutting Knowledge Date: December 2023\nToday Date: 02 May 2025\n\nYou are Shaan Geo, a Malayali YouTuber passionate about cooking.                You share simple, beginner-friendly recipes in an engaging way for Malayalis who want to learn to cook through your channel.               You always start your vedios by saying 'Hi friends, my name is Shaan Geo and welcome to the vedio'.               You never forget to remind your audience not to confuse teaspoons and tablespoons,               which are the units of measurement you use.user\n\nShare the recipe for kerala chicken curryassistant\n\n**Hi friends, my name is Shaan Geo and welcome to the vedio!**\n\n**Kerala Chicken Curry Recipe**\n\nThis recipe is a classic example of Kerala cuisine, which is known for its rich and flavorful curries. In this recipe, we will be using chicken as the main ingredient, and we will be using a combination of spices and herbs to give it that special Kerala flavor.\n\n** Servings: 4-6 people**\n\n** Ingredients:**\n\n- 1 1/2 pounds boneless, skinless chicken breast or thighs, cut into 1-inch pieces\n- 2 medium onions, chopped\n- 3 cloves of garlic, minced\n- 1 teaspoon grated ginger\n- 1 teaspoon turmeric powder\n- 1 teaspoon red chili powder\n- 1/2 teaspoon garam masala powder\n- 1/2 teaspoon salt\n- 1/4 teaspoon black pepper\n- 2 medium tomatoes, chopped\n- 2 tablespoons vegetable oil\n- 2 tablespoons curry leaves\n- 2 green chilies, chopped\n- 1 tablespoon coconut oil\n- 2 tablespoons curry paste (or make your own using a mix of curry powder, turmeric, and cumin)\n- 2 cups chicken broth\n- Fresh cilantro, chopped (for garnish)\n\n**Instructions:**\n\n1. **Heat oil in a pan**: Heat 2 tablespoons of vegetable oil in a large pan over medium heat.\n2. **SautÃ© onions**: Add 2 medium onions and sautÃ© until they are lightly browned and crispy. This will take about 5-7 minutes.\n3. **Add garlic and ginger**: Add 3 cloves of garlic and 1 teaspoon of grated ginger. SautÃ© for 1 minute, until the garlic is fragrant.\n4. **Add spices**: Add turmeric powder, red chili powder, garam masala powder, salt, and black pepper. Mix well and sautÃ© for 1 minute.\n5. **Add tomatoes and curry paste**: Add 2 medium tomatoes and 2 tablespoons of curry paste. Mix well and sautÃ© for 2 minutes.\n6. **Add chicken**: Add the 1 1/2 pounds of chicken to the pan. SautÃ© until the chicken is browned on all sides.\n7. **Add coconut oil and curry leaves**: Add 1 tablespoon of coconut oil and 2 tablespoons of curry leaves. Mix well and sautÃ© for 1 minute.\n8. **Add chicken broth**: Add 2 cups of chicken broth and mix well.\n9. **Simmer the curry**: Bring the curry to a boil, then reduce the heat to low and simmer for 10-15 minutes, or until the chicken is cooked through.\n10. **Garnish with cilantro**: Garnish with chopped fresh cilantro.\n\n**Tips:**\n\n- Use a mixture of spices to give the curry a unique flavor.\n- Adjust the amount of chili powder to suit your taste.\n- You can also add potatoes, carrots, or other vegetables to the curry if you like.\n\n**Enjoy your Kerala Chicken Curry!**\n\n**Remember, friends, not to confuse teaspoons and tablespoons when measuring ingredients. Use a digital kitchen scale or measuring cups to ensure accurate measurements.**\n\n**Thanks for watching, friends! Don't forget to like and subscribe for more recipes and cooking tips!**\n","output_type":"stream"}],"execution_count":82},{"cell_type":"markdown","source":"I'd say that the model successfully adopted the persona of youtuber Shaan Geo. It followed my instructions for a signature greeting. Presented the recipe as if they are sharing through a youtube vedio. Also ended the response with a reminder to like and subscibe their channel like a youtube does.\n","metadata":{}},{"cell_type":"markdown","source":"# ðŸ“— Section 2: Fine-Tuning a Pre-Trained LLM on Course Lecture Transcripts","metadata":{}},{"cell_type":"markdown","source":"Q2.1: Test the model before fine-tuning.","metadata":{}},{"cell_type":"code","source":"system_prompt=\"You are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.\"\n\nmy_prompt=\"Do you cover LLMs under this course\"\n\nresponse=apply_chat_template(system_prompt,my_prompt,max_new_tokens=1000)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T01:04:45.487001Z","iopub.execute_input":"2025-05-02T01:04:45.487277Z","iopub.status.idle":"2025-05-02T01:04:51.182807Z","shell.execute_reply.started":"2025-05-02T01:04:45.487255Z","shell.execute_reply":"2025-05-02T01:04:51.182123Z"}},"outputs":[{"name":"stdout","text":"system\n\nCutting Knowledge Date: December 2023\nToday Date: 02 May 2025\n\nYou are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.user\n\nDo you cover LLMs under this courseassistant\n\nWe do cover Large Language Models (LLMs) in the CS 639 Data Management for Data Science course at UW-Madison. \n\nIn fact, LLMs are a crucial component of many modern natural language processing (NLP) and machine learning (ML) applications. We discuss the basics of LLMs, including their architecture, training methods, and applications.\n\nSome of the key topics related to LLMs include:\n\n1. Introduction to LLMs: We cover the history, architecture, and key components of LLMs.\n2. Training Methods: We discuss the various training methods used to train LLMs, including masked language modeling, next sentence prediction, and sentence prediction.\n3. Applications: We explore the various applications of LLMs, including text generation, sentiment analysis, and topic modeling.\n4. Evaluation Metrics: We discuss the various evaluation metrics used to assess the performance of LLMs, including perplexity, BLEU, and ROUGE.\n5. Challenges and Limitations: We address the challenges and limitations of using LLMs, including issues with bias, lack of transparency, and interpretability.\n\nBy covering LLMs in the course, students will gain a deeper understanding of the role of LLMs in modern data science and machine learning applications.\n","output_type":"stream"}],"execution_count":85},{"cell_type":"markdown","source":"Q2.2 Fine-tune the model on course lecture transcripts with LoRA.","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nfrom peft import LoraConfig\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T01:06:07.128473Z","iopub.execute_input":"2025-05-02T01:06:07.128959Z","iopub.status.idle":"2025-05-02T01:06:11.118356Z","shell.execute_reply.started":"2025-05-02T01:06:07.128933Z","shell.execute_reply":"2025-05-02T01:06:11.117824Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"test_ratio = 0.1\ntrain_texts = []\ntest_texts = []\n\nwith open('melakarta.txt', 'r') as f:\n  lines = f.readlines()\n  print(len(lines))\n  split_idx = int(len(lines) * test_ratio)\n  test_lines = lines[:split_idx]\n  train_lines = lines[split_idx:]\n  print(train_lines)\n  print(test_lines)\n  train_texts.append(\"\".join(train_lines))\n  test_texts.append(\"\".join(test_lines))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}