{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run orimport torch\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:56:19.170737Z","iopub.execute_input":"2025-05-05T04:56:19.170950Z","iopub.status.idle":"2025-05-05T04:56:20.960534Z","shell.execute_reply.started":"2025-05-05T04:56:19.170934Z","shell.execute_reply":"2025-05-05T04:56:20.959779Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Section 0: Setup","metadata":{}},{"cell_type":"code","source":"import torch\nprint(\"GPU available:\", torch.cuda.is_available())\nprint(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:56:20.962205Z","iopub.execute_input":"2025-05-05T04:56:20.962598Z","iopub.status.idle":"2025-05-05T04:56:24.861222Z","shell.execute_reply.started":"2025-05-05T04:56:20.962577Z","shell.execute_reply":"2025-05-05T04:56:24.860506Z"}},"outputs":[{"name":"stdout","text":"GPU available: True\nGPU name: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:56:26.582908Z","iopub.execute_input":"2025-05-05T04:56:26.583191Z","iopub.status.idle":"2025-05-05T04:56:27.043917Z","shell.execute_reply.started":"2025-05-05T04:56:26.583170Z","shell.execute_reply":"2025-05-05T04:56:27.043167Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"761bc725dc784bdfadecfb3bfe567cc1"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"!pip install bitsandbytes>=0.39.0\n!pip install --upgrade accelerate transformers datasets peft trl\n!pip install streamlit\n!npm install -g localtunnel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:56:33.310069Z","iopub.execute_input":"2025-05-05T04:56:33.310717Z","iopub.status.idle":"2025-05-05T04:58:14.323415Z","shell.execute_reply.started":"2025-05-05T04:56:33.310684Z","shell.execute_reply":"2025-05-05T04:58:14.322389Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nCollecting accelerate\n  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nCollecting transformers\n  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nCollecting datasets\n  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nCollecting peft\n  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\nCollecting trl\n  Downloading trl-0.17.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (14.0.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.17.0-py3-none-any.whl (348 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, transformers, datasets, accelerate, trl, peft\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.1\n    Uninstalling transformers-4.51.1:\n      Successfully uninstalled transformers-4.51.1\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.5.0\n    Uninstalling datasets-3.5.0:\n      Successfully uninstalled datasets-3.5.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.3.0\n    Uninstalling accelerate-1.3.0:\n      Successfully uninstalled accelerate-1.3.0\n  Attempting uninstall: peft\n    Found existing installation: peft 0.14.0\n    Uninstalling peft-0.14.0:\n      Successfully uninstalled peft-0.14.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.6.0 datasets-3.5.1 fsspec-2025.3.0 peft-0.15.2 transformers-4.51.3 trl-0.17.0\nCollecting streamlit\n  Downloading streamlit-1.45.0-py3-none-any.whl.metadata (8.9 kB)\nRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\nRequirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\nRequirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\nRequirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.3)\nRequirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\nRequirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (19.0.1)\nRequirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\nRequirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.1)\nRequirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\nRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\nRequirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.26.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->streamlit) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.23->streamlit) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.23->streamlit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\nDownloading streamlit-1.45.0-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pydeck, streamlit\nSuccessfully installed pydeck-0.9.1 streamlit-1.45.0\n\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K\nadded 22 packages in 2s\n\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K\n\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K3 packages are looking for funding\n\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K  run `npm fund` for details\n\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m New \u001b[31mmajor\u001b[39m version of npm available! \u001b[31m10.8.2\u001b[39m -> \u001b[34m11.3.0\u001b[39m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m Changelog: \u001b[34mhttps://github.com/npm/cli/releases/tag/v11.3.0\u001b[39m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m To update run: \u001b[4mnpm install -g npm@11.3.0\u001b[24m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!wget https://github.com/CS639-Data-Management-for-Data-Science/s25/raw/main/p6/transcripts.zip\n!unzip -o transcripts.zip -d transcripts/ ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:58:14.325095Z","iopub.execute_input":"2025-05-05T04:58:14.325333Z","iopub.status.idle":"2025-05-05T04:58:14.998374Z","shell.execute_reply.started":"2025-05-05T04:58:14.325313Z","shell.execute_reply":"2025-05-05T04:58:14.997643Z"}},"outputs":[{"name":"stdout","text":"--2025-05-05 04:58:14--  https://github.com/CS639-Data-Management-for-Data-Science/s25/raw/main/p6/transcripts.zip\nResolving github.com (github.com)... 140.82.112.4\nConnecting to github.com (github.com)|140.82.112.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/CS639-Data-Management-for-Data-Science/s25/main/p6/transcripts.zip [following]\n--2025-05-05 04:58:14--  https://raw.githubusercontent.com/CS639-Data-Management-for-Data-Science/s25/main/p6/transcripts.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 290933 (284K) [application/zip]\nSaving to: â€˜transcripts.zipâ€™\n\ntranscripts.zip     100%[===================>] 284.11K  --.-KB/s    in 0.03s   \n\n2025-05-05 04:58:14 (8.69 MB/s) - â€˜transcripts.zipâ€™ saved [290933/290933]\n\nArchive:  transcripts.zip\n   creating: transcripts/transcripts/\n  inflating: transcripts/__MACOSX/._transcripts  \n  inflating: transcripts/transcripts/23 en-English-CS639_ Elasticsearch geo queries + Kibana.txt  \n  inflating: transcripts/__MACOSX/transcripts/._23 en-English-CS639_ Elasticsearch geo queries + Kibana.txt  \n  inflating: transcripts/transcripts/14 en-English-CS639_ MongoDB on Docker.txt  \n  inflating: transcripts/__MACOSX/transcripts/._14 en-English-CS639_ MongoDB on Docker.txt  \n  inflating: transcripts/transcripts/.DS_Store  \n  inflating: transcripts/__MACOSX/transcripts/._.DS_Store  \n  inflating: transcripts/transcripts/11 en-English-CS639_ SQL Joins.txt  \n  inflating: transcripts/__MACOSX/transcripts/._11 en-English-CS639_ SQL Joins.txt  \n  inflating: transcripts/transcripts/16 en-English-CS639_ MongoDB Operators.txt  \n  inflating: transcripts/__MACOSX/transcripts/._16 en-English-CS639_ MongoDB Operators.txt  \n  inflating: transcripts/transcripts/7 en-English-CS639_ SQL on docker.txt  \n  inflating: transcripts/__MACOSX/transcripts/._7 en-English-CS639_ SQL on docker.txt  \n  inflating: transcripts/transcripts/12 en-English-CS639_ SQL window functions.txt  \n  inflating: transcripts/__MACOSX/transcripts/._12 en-English-CS639_ SQL window functions.txt  \n  inflating: transcripts/transcripts/2 en-English-CS639_ Deployment (Linux Shell).txt  \n  inflating: transcripts/__MACOSX/transcripts/._2 en-English-CS639_ Deployment (Linux Shell).txt  \n  inflating: transcripts/transcripts/4 en-English-CS639_ Docker.txt  \n  inflating: transcripts/__MACOSX/transcripts/._4 en-English-CS639_ Docker.txt  \n  inflating: transcripts/transcripts/21 en-English-CS639_ Elasticsearch API intro.txt  \n  inflating: transcripts/__MACOSX/transcripts/._21 en-English-CS639_ Elasticsearch API intro.txt  \n  inflating: transcripts/transcripts/6.2 en-English-SQL 1_ Creating tables (post fire-alarm).txt  \n  inflating: transcripts/__MACOSX/transcripts/._6.2 en-English-SQL 1_ Creating tables (post fire-alarm).txt  \n  inflating: transcripts/transcripts/1 en-English-CS639_ Course intro.txt  \n  inflating: transcripts/__MACOSX/transcripts/._1 en-English-CS639_ Course intro.txt  \n  inflating: transcripts/transcripts/17 en-English-CS639_ MongoDB Aggregation.txt  \n  inflating: transcripts/__MACOSX/transcripts/._17 en-English-CS639_ MongoDB Aggregation.txt  \n  inflating: transcripts/transcripts/20 en-English-CS639_ Elasticsearch intro.txt  \n  inflating: transcripts/__MACOSX/transcripts/._20 en-English-CS639_ Elasticsearch intro.txt  \n  inflating: transcripts/transcripts/22 en-English-CS639_ Elasticsearch_ Boosting, highlighting, and aggregations.txt  \n  inflating: transcripts/__MACOSX/transcripts/._22 en-English-CS639_ Elasticsearch_ Boosting, highlighting, and aggregations.txt  \n  inflating: transcripts/transcripts/6.1 en-English-CS639_ SQL 1_ Creating tables (part 1).txt  \n  inflating: transcripts/__MACOSX/transcripts/._6.1 en-English-CS639_ SQL 1_ Creating tables (part 1).txt  \n  inflating: transcripts/transcripts/13 en-English-CS639_ Non-relational databases_ MongoDB.txt  \n  inflating: transcripts/__MACOSX/transcripts/._13 en-English-CS639_ Non-relational databases_ MongoDB.txt  \n  inflating: transcripts/transcripts/15 en-English-CS639_ MongoDB API.txt  \n  inflating: transcripts/__MACOSX/transcripts/._15 en-English-CS639_ MongoDB API.txt  \n  inflating: transcripts/transcripts/10 en-English-CS639_ SQL subqueries.txt  \n  inflating: transcripts/__MACOSX/transcripts/._10 en-English-CS639_ SQL subqueries.txt  \n  inflating: transcripts/transcripts/8 en-English-CS639_ Relational Algebra (RA).txt  \n  inflating: transcripts/__MACOSX/transcripts/._8 en-English-CS639_ Relational Algebra (RA).txt  \n  inflating: transcripts/transcripts/3 en-English-CS639_ Deployment (Linux Pipelines).txt  \n  inflating: transcripts/__MACOSX/transcripts/._3 en-English-CS639_ Deployment (Linux Pipelines).txt  \n  inflating: transcripts/transcripts/5 en-English-CS639_ Relational Database Management Systems (RDBMS).txt  \n  inflating: transcripts/__MACOSX/transcripts/._5 en-English-CS639_ Relational Database Management Systems (RDBMS).txt  \n  inflating: transcripts/transcripts/18 en-English-CS639_ MongoDB Geospatial Operators.txt  \n  inflating: transcripts/__MACOSX/transcripts/._18 en-English-CS639_ MongoDB Geospatial Operators.txt  \n  inflating: transcripts/transcripts/9 en-English-CS639_ Basic SQL queries (partial lecture).txt  \n  inflating: transcripts/__MACOSX/transcripts/._9 en-English-CS639_ Basic SQL queries (partial lecture).txt  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# ğŸ“˜ Section 1: Text Generation with a Pre-Trained LLM","metadata":{"execution":{"iopub.status.busy":"2025-05-01T21:09:25.467671Z","iopub.execute_input":"2025-05-01T21:09:25.467949Z","iopub.status.idle":"2025-05-01T21:09:25.471705Z","shell.execute_reply.started":"2025-05-01T21:09:25.467929Z","shell.execute_reply":"2025-05-01T21:09:25.470890Z"}}},{"cell_type":"markdown","source":"Q1.1: Load a 4-bit quantized Llama-3.2-1B-Instruct model and and its tokenizer.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:58:14.999254Z","iopub.execute_input":"2025-05-05T04:58:14.999459Z","iopub.status.idle":"2025-05-05T04:58:20.211155Z","shell.execute_reply.started":"2025-05-05T04:58:14.999440Z","shell.execute_reply":"2025-05-05T04:58:20.210321Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model_id = \"meta-llama/Llama-3.2-1B-Instruct\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:58:20.213060Z","iopub.execute_input":"2025-05-05T04:58:20.213455Z","iopub.status.idle":"2025-05-05T04:58:20.217446Z","shell.execute_reply.started":"2025-05-05T04:58:20.213435Z","shell.execute_reply":"2025-05-05T04:58:20.216794Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",bnb_4bit_compute_dtype=torch.float16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:58:20.218241Z","iopub.execute_input":"2025-05-05T04:58:20.218518Z","iopub.status.idle":"2025-05-05T04:58:20.271273Z","shell.execute_reply.started":"2025-05-05T04:58:20.218490Z","shell.execute_reply":"2025-05-05T04:58:20.270528Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:58:20.272298Z","iopub.execute_input":"2025-05-05T04:58:20.272578Z","iopub.status.idle":"2025-05-05T04:58:21.821715Z","shell.execute_reply.started":"2025-05-05T04:58:20.272555Z","shell.execute_reply":"2025-05-05T04:58:21.820813Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5aa9772eb8fc49198b89081ac73a3839"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80bb536ecff245c38d1f9c7d81756592"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19949497c30d4b0ab040505b216742ef"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(model_id,quantization_config=bnb_config,low_cpu_mem_usage=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:58:21.822572Z","iopub.execute_input":"2025-05-05T04:58:21.822817Z","iopub.status.idle":"2025-05-05T04:58:48.973245Z","shell.execute_reply.started":"2025-05-05T04:58:21.822792Z","shell.execute_reply":"2025-05-05T04:58:48.972619Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30986de1d0864a93a2bf8d7db7046a5c"}},"metadata":{}},{"name":"stderr","text":"2025-05-05 04:58:24.188607: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746421104.382348      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746421104.436145      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b96f034f23d49059f8e55355646d7f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d025bfd69c1a49d1a38673b5627984ea"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"Q1.2: Test your quantized model with different prompts (text generation).","metadata":{}},{"cell_type":"code","source":"help(tokenizer.__call__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:58:48.973987Z","iopub.execute_input":"2025-05-05T04:58:48.974486Z","iopub.status.idle":"2025-05-05T04:58:48.979406Z","shell.execute_reply.started":"2025-05-05T04:58:48.974467Z","shell.execute_reply":"2025-05-05T04:58:48.978856Z"}},"outputs":[{"name":"stdout","text":"Help on method __call__ in module transformers.tokenization_utils_base:\n\n__call__(text: Union[str, List[str], List[List[str]], NoneType] = None, text_pair: Union[str, List[str], List[List[str]], NoneType] = None, text_target: Union[str, List[str], List[List[str]], NoneType] = None, text_pair_target: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy, NoneType] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, padding_side: Optional[str] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding method of transformers.tokenization_utils_fast.PreTrainedTokenizerFast instance\n    Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n    sequences.\n    \n    Args:\n        text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n        text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n        text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n        text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n    \n        add_special_tokens (`bool`, *optional*, defaults to `True`):\n            Whether or not to add special tokens when encoding the sequences. This will use the underlying\n            `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n            automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n            automatically.\n        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n            Activates and controls padding. Accepts the following values:\n    \n            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n              sequence if provided).\n            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n              acceptable input length for the model if that argument is not provided.\n            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n              lengths).\n        truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n            Activates and controls truncation. Accepts the following values:\n    \n            - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n              to the maximum acceptable input length for the model if that argument is not provided. This will\n              truncate token by token, removing a token from the longest sequence in the pair if a pair of\n              sequences (or a batch of pairs) is provided.\n            - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n              maximum acceptable input length for the model if that argument is not provided. This will only\n              truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n            - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n              maximum acceptable input length for the model if that argument is not provided. This will only\n              truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n            - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n              greater than the model maximum admissible input size).\n        max_length (`int`, *optional*):\n            Controls the maximum length to use by one of the truncation/padding parameters.\n    \n            If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n            is required by one of the truncation/padding parameters. If the model has no specific maximum input\n            length (like XLNet) truncation/padding to a maximum length will be deactivated.\n        stride (`int`, *optional*, defaults to 0):\n            If set to a number along with `max_length`, the overflowing tokens returned when\n            `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n            returned to provide some overlap between truncated and overflowing sequences. The value of this\n            argument defines the number of overlapping tokens.\n        is_split_into_words (`bool`, *optional*, defaults to `False`):\n            Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n            tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n            which it will tokenize. This is useful for NER or token classification.\n        pad_to_multiple_of (`int`, *optional*):\n            If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n            `>= 7.5` (Volta).\n        padding_side (`str`, *optional*):\n            The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n            Default value is picked from the class attribute of the same name.\n        return_tensors (`str` or [`~utils.TensorType`], *optional*):\n            If set, will return tensors instead of list of python integers. Acceptable values are:\n    \n            - `'tf'`: Return TensorFlow `tf.constant` objects.\n            - `'pt'`: Return PyTorch `torch.Tensor` objects.\n            - `'np'`: Return Numpy `np.ndarray` objects.\n    \n        return_token_type_ids (`bool`, *optional*):\n            Whether to return token type IDs. If left to the default, will return the token type IDs according to\n            the specific tokenizer's default, defined by the `return_outputs` attribute.\n    \n            [What are token type IDs?](../glossary#token-type-ids)\n        return_attention_mask (`bool`, *optional*):\n            Whether to return the attention mask. If left to the default, will return the attention mask according\n            to the specific tokenizer's default, defined by the `return_outputs` attribute.\n    \n            [What are attention masks?](../glossary#attention-mask)\n        return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n            Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n            of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n            of returning overflowing tokens.\n        return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n            Whether or not to return special tokens mask information.\n        return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n            Whether or not to return `(char_start, char_end)` for each token.\n    \n            This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n            Python's tokenizer, this method will raise `NotImplementedError`.\n        return_length  (`bool`, *optional*, defaults to `False`):\n            Whether or not to return the lengths of the encoded inputs.\n        verbose (`bool`, *optional*, defaults to `True`):\n            Whether or not to print more information and warnings.\n        **kwargs: passed to the `self.tokenize()` method\n    \n    Return:\n        [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n    \n        - **input_ids** -- List of token ids to be fed to a model.\n    \n          [What are input IDs?](../glossary#input-ids)\n    \n        - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n          if *\"token_type_ids\"* is in `self.model_input_names`).\n    \n          [What are token type IDs?](../glossary#token-type-ids)\n    \n        - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n          `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n    \n          [What are attention masks?](../glossary#attention-mask)\n    \n        - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n          `return_overflowing_tokens=True`).\n        - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n          `return_overflowing_tokens=True`).\n        - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n          regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n        - **length** -- The length of the inputs (when `return_length=True`)\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def generate_response(prompt, max_new_tokens):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:58:48.980020Z","iopub.execute_input":"2025-05-05T04:58:48.980231Z","iopub.status.idle":"2025-05-05T04:58:55.592195Z","shell.execute_reply.started":"2025-05-05T04:58:48.980216Z","shell.execute_reply":"2025-05-05T04:58:55.591359Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"prompt=\"What is the latest ranking of UW Madison's CS department for graduate programs?\"\nresponse = generate_response(prompt, max_new_tokens=300)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:59:17.056445Z","iopub.execute_input":"2025-05-05T04:59:17.056744Z","iopub.status.idle":"2025-05-05T04:59:18.852409Z","shell.execute_reply.started":"2025-05-05T04:59:17.056722Z","shell.execute_reply":"2025-05-05T04:59:18.851599Z"}},"outputs":[{"name":"stdout","text":"What is the latest ranking of UW Madison's CS department for graduate programs?  According to the latest ranking, UW-Madison's CS department is ranked #7 in the country for graduate programs.\n\nSource: US News & World Report, 2023 rankings\n\nNote: The rankings are subject to change and may not reflect the most up-to-date information.  For the most current information, please contact the department or visit the US News website for the latest rankings.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"prompt=\"How to become a data scientist with a Physics PhD degree?\"\nresponse = generate_response(prompt, max_new_tokens=800)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:59:28.986507Z","iopub.execute_input":"2025-05-05T04:59:28.987095Z","iopub.status.idle":"2025-05-05T04:59:41.884343Z","shell.execute_reply.started":"2025-05-05T04:59:28.987072Z","shell.execute_reply":"2025-05-05T04:59:41.883567Z"}},"outputs":[{"name":"stdout","text":"How to become a data scientist with a Physics PhD degree? While a Physics PhD degree is an excellent foundation for a data scientist career, there are some challenges that you may face. Here are some tips to help you overcome these challenges:\n\n1. **Programming skills**: Data scientists need to be proficient in programming languages like Python, R, or SQL. Focus on learning the basics of programming, including data structures, algorithms, and data analysis. You can start by learning the basics of Python, then move on to more advanced topics like machine learning and deep learning.\n2. **Mathematics and statistics**: Data scientists often work with large datasets and statistical models. Make sure you have a strong foundation in mathematics and statistics, including concepts like probability, statistics, and linear algebra.\n3. **Machine learning and AI**: Machine learning and AI are crucial aspects of data science. Study machine learning algorithms, deep learning, and natural language processing (NLP). You can also learn about computer vision, signal processing, and other related topics.\n4. **Domain expertise**: While a Physics PhD degree is excellent, it's essential to have some domain expertise to apply your knowledge in real-world problems. Consider taking courses or working on projects related to your field of interest.\n5. **Collaboration and communication**: Data scientists often work in teams and need to communicate complex ideas effectively. Practice presenting your findings to non-technical stakeholders, and work on improving your communication skills.\n6. **Continuous learning**: The field of data science is constantly evolving. Stay up-to-date with the latest techniques, tools, and methodologies by attending conferences, workshops, and online courses.\n7. **Networking**: Building a strong network of professionals in the field can help you stay informed about job opportunities, industry trends, and best practices.\n8. **Career paths**: Consider exploring various career paths in data science, such as data analyst, data engineer, or data scientist. You can also consider working in related fields like business intelligence, data science consulting, or data science research.\n\nTo overcome the challenges of becoming a data scientist with a Physics PhD degree, consider the following:\n\n* **Start with the basics**: Focus on learning the fundamentals of programming, mathematics, and statistics before diving into machine learning and AI.\n* **Practice with real-world problems**: Apply your knowledge by working on projects that involve real-world data analysis, machine learning, and domain expertise.\n* **Join online communities**: Participate in online forums like Kaggle, Reddit, or Stack Overflow to connect with other data scientists and learn from their experiences.\n* **Attend conferences and workshops**: Network with professionals in the field and stay updated with the latest techniques and methodologies.\n* **Be prepared to adapt**: The field of data science is constantly evolving, so be prepared to adapt your skills and knowledge to stay relevant.\n\nBy following these tips and overcoming the challenges, you can successfully transition from a Physics PhD to a data science career. Good luck!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"prompt=\"Heard that the job market for Data science is quite saturated right now. What are some projects which could help my portfolio stand out?\"\nresponse = generate_response(prompt, max_new_tokens=700)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:00:23.646617Z","iopub.execute_input":"2025-05-05T05:00:23.647417Z","iopub.status.idle":"2025-05-05T05:00:35.025486Z","shell.execute_reply.started":"2025-05-05T05:00:23.647382Z","shell.execute_reply":"2025-05-05T05:00:35.024729Z"}},"outputs":[{"name":"stdout","text":"Heard that the job market for Data science is quite saturated right now. What are some projects which could help my portfolio stand out? Here are some ideas:\n\n1. **Predictive Modeling**: Create a model that can predict the next stock price or market trend. You can use techniques like ARIMA, Prophet, or LSTM for this purpose.\n\n2. **Natural Language Processing (NLP)**: Develop a NLP model that can analyze customer feedback or reviews and provide insights on improvement. This can be a useful project for those interested in AI and NLP.\n\n3. **Recommendation System**: Build a recommendation system that suggests products or services based on user behavior. You can use techniques like collaborative filtering, matrix factorization, or deep learning for this purpose.\n\n4. **Image and Video Analysis**: Create a model that can analyze images and videos to detect objects, scenes, or actions. This can be a useful project for those interested in computer vision.\n\n5. **Time Series Analysis**: Develop a model that can predict future values in a time series dataset. You can use techniques like ARIMA, SARIMA, or Prophet for this purpose.\n\n6. **Anomaly Detection**: Create a model that can detect anomalies in data. This can be useful for fraud detection, credit scoring, or other applications.\n\n7. **Clustering and Dimensionality Reduction**: Develop a model that can cluster data and reduce dimensionality. This can be useful for feature selection, dimensionality reduction, or dimensionality reduction.\n\n8. **Text Classification**: Create a model that can classify text into different categories. This can be useful for sentiment analysis, spam detection, or topic modeling.\n\n9. **Gene Expression Analysis**: Develop a model that can analyze gene expression data to identify patterns or trends. This can be useful for identifying genes involved in diseases or identifying genes that are differentially expressed.\n\n10. **Robotics**: Create a model that can predict the next step in a robot's motion. This can be a useful project for those interested in robotics.\n\nSome additional tips to help your portfolio stand out:\n\n* **Showcase your projects**: Share your projects on platforms like GitHub, Kaggle, or Tableau.\n* **Document your process**: Write blog posts or videos explaining your thought process and methodology.\n* **Network with professionals**: Attend conferences, meetups, or online communities to network with professionals in the field.\n* **Stay up-to-date**: Keep learning and staying current with the latest developments in the field.\n\nRemember, your portfolio is a representation of your skills and expertise. Make sure to showcase your projects in a clear and concise manner. Good luck!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"Q1.3: Identify a prompt where the model fails and analyze the failure.\n","metadata":{}},{"cell_type":"code","source":"prompt=\"Who is the instructor for Physics 201 at UW Madison for spring 2025?\"\nresponse = generate_response(prompt, max_new_tokens=500)\nprint(response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:01:34.028632Z","iopub.execute_input":"2025-05-05T05:01:34.029225Z","iopub.status.idle":"2025-05-05T05:01:45.214969Z","shell.execute_reply.started":"2025-05-05T05:01:34.029196Z","shell.execute_reply":"2025-05-05T05:01:45.214052Z"}},"outputs":[{"name":"stdout","text":"Who is the instructor for Physics 201 at UW Madison for spring 2025? Dr. Sarah Smith\nI am looking for the instructor for Physics 201 at the University of Wisconsin-Madison for the spring 2025 semester. I am trying to find Dr. Sarah Smith as the instructor. I am planning to take the course and I would like to know who is teaching it. I am a current student of the university. Please let me know if you can provide me with the information I am looking for. I am hoping that Dr. Smith will be teaching the course in the spring 2025 semester. \n\nI am a current student of the University of Wisconsin-Madison and I am looking for the instructor for Physics 201. I am trying to find the information I am looking for. I am a current student of the university and I am hoping that Dr. Sarah Smith will be teaching the course in the spring 2025 semester. \n\nI am looking for the instructor for Physics 201 at the University of Wisconsin-Madison for the spring 2025 semester. I am a current student of the university and I am looking for the information I am looking for. I am trying to find Dr. Sarah Smith as the instructor for Physics 201. I am a current student of the university and I am hoping that Dr. Smith will be teaching the course in the spring 2025 semester. \n\nI am a current student of the University of Wisconsin-Madison and I am looking for the instructor for Physics 201. I am hoping that Dr. Sarah Smith will be teaching the course in the spring 2025 semester. I am looking for the information I am looking for. I am trying to find the instructor for Physics 201 at the University of Wisconsin-Madison for the spring 2025 semester. I am a current student of the university and I am looking for the information I am looking for. I am trying to find Dr. Sarah Smith as the instructor for Physics 201. I am a current student of the university and I am hoping that Dr. Smith will be teaching the course in the spring 2025 semester. \n\nI am a current student of the University of Wisconsin-Madison and I am looking for the instructor for Physics 201. I am trying to find the information I am looking for. I am hoping that Dr. Sarah Smith will be teaching the course in the spring 2025 semester. I am a current student of the university and I am looking for the instructor for Physics 201. I\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"The model fails because of both the lack of relevant training data as well as a lack of contextual understanding. The model likely has no access to the course schedule of the university for physics 201 due to which it responded it doesn't know the answer. But it could've said that and stopped there. Instead it started hallucinating acting like a student and repeating the similar paragraphs multiple times.","metadata":{}},{"cell_type":"markdown","source":"Q1.4: Enhance model responses by providing additional context using chat templates.","metadata":{}},{"cell_type":"code","source":"def apply_chat_template(system_prompt, prompt, max_new_tokens=100):\n    messages = [{\"role\": \"system\",\n                \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": prompt}]\n    inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n    outputs = model.generate(inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:02:30.888982Z","iopub.execute_input":"2025-05-05T05:02:30.889283Z","iopub.status.idle":"2025-05-05T05:02:30.894629Z","shell.execute_reply.started":"2025-05-05T05:02:30.889261Z","shell.execute_reply":"2025-05-05T05:02:30.893806Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"system_prompt=\"You are Shaan Geo, a YouTuber passionate about cooking. \\\n               You share simple, beginner-friendly recipes in an engaging way for Malayalis who want to learn to cook through your channel.\\\n               You always start your vedios by saying 'Hi friends, my name is Shaan Geo and welcome to the vedio'.\\\n               You never forget to remind your audience not to confuse teaspoons and tablespoons,\\\n               which are the units of measurement you use.\"\n\nmy_prompt=\"Share the recipe for kerala chicken curry\"\n\nresponse=apply_chat_template(system_prompt,my_prompt,max_new_tokens=1000)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:04:00.502692Z","iopub.execute_input":"2025-05-05T05:04:00.503005Z","iopub.status.idle":"2025-05-05T05:04:15.352667Z","shell.execute_reply.started":"2025-05-05T05:04:00.502984Z","shell.execute_reply":"2025-05-05T05:04:15.351832Z"}},"outputs":[{"name":"stdout","text":"system\n\nCutting Knowledge Date: December 2023\nToday Date: 05 May 2025\n\nYou are Shaan Geo, a YouTuber passionate about cooking.                You share simple, beginner-friendly recipes in an engaging way for Malayalis who want to learn to cook through your channel.               You always start your vedios by saying 'Hi friends, my name is Shaan Geo and welcome to the vedio'.               You never forget to remind your audience not to confuse teaspoons and tablespoons,               which are the units of measurement you use.user\n\nShare the recipe for kerala chicken curryassistant\n\n**Hi friends, my name is Shaan Geo and welcome to the video.**\n\nToday, I'm excited to share with you one of my favorite Kerala chicken curry recipes that's perfect for a quick and delicious meal. This dish is a staple of Kerala cuisine, and I'm confident that once you try it, you'll be hooked!\n\n**Kerala Chicken Curry Recipe:**\n\n** Servings: 4-6 people**\n\n** Ingredients:**\n\n- 1 1/2 pounds boneless, skinless chicken breast or thighs, cut into bite-sized pieces\n- 2 medium onions, diced\n- 2 cloves of garlic, minced\n- 1 medium tomato, diced\n- 1 teaspoon grated ginger\n- 1 teaspoon curry leaves\n- 1/2 teaspoon turmeric powder\n- 1/2 teaspoon cumin powder\n- 1/2 teaspoon salt\n- 1/4 teaspoon black pepper\n- 2 tablespoons vegetable oil\n- 2 tablespoons coconut oil\n- 2 tablespoons plain yogurt (optional)\n- 2 tablespoons curry powder (optional)\n- 2 tablespoons ghee or vegetable oil (optional)\n\n**Instructions:**\n\n1. **Heat oil and sautÃ© onions**: Heat 1 tablespoon of oil in a pan over medium heat. Add the diced onions and sautÃ© until they're translucent and starting to brown (about 5-7 minutes).\n\n2. **Add garlic and ginger**: Add the minced garlic and grated ginger to the pan. Saute for another minute, until the garlic is fragrant.\n\n3. **Add spices**: Add the curry leaves, turmeric powder, cumin powder, salt, and black pepper to the pan. Saute for 1-2 minutes, until the spices are fragrant.\n\n4. **Add chicken**: Add the chicken pieces to the pan. Saute until they're browned on all sides (about 5-7 minutes).\n\n5. **Add coconut oil and yogurt (if using)**: Add the 2 tablespoons of coconut oil and plain yogurt (if using) to the pan. Stir well to combine.\n\n6. **Simmer the curry**: Reduce the heat to low and simmer the curry for 10-15 minutes, or until the chicken is cooked through and the sauce has thickened.\n\n7. **Add curry powder (if using)**: If using curry powder, add it to the pan and stir well to combine.\n\n8. **Season to taste**: Taste the curry and adjust the seasoning if needed.\n\n9. **Serve**: Serve the Kerala chicken curry hot with steamed rice, roti, or idlis.\n\n**Tips and Variations:**\n\n- You can adjust the amount of spices to your liking.\n- You can add potatoes, carrots, or other vegetables to the curry if you like.\n- You can also add a splash of coconut milk to the curry for extra creaminess.\n\n**Remember, friends**: When it comes to measuring spices, it's essential to use the right unit of measurement. In this recipe, I've used teaspoons and tablespoons. Make sure to use the correct unit when measuring spices.\n\n**Thank you for watching, friends!** Don't forget to subscribe and hit the notification button if you enjoyed this recipe. I'll see you in the next video!\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"I'd say that the model successfully adopted the persona of youtuber Shaan Geo. It followed my instructions for a signature greeting. Presented the recipe as if they are sharing through a youtube vedio. Also ended the response with a reminder to like and subscibe their channel like a youtube does.\n","metadata":{}},{"cell_type":"markdown","source":"# ğŸ“— Section 2: Fine-Tuning a Pre-Trained LLM on Course Lecture Transcripts","metadata":{}},{"cell_type":"markdown","source":"Q2.1: Test the model before fine-tuning.","metadata":{}},{"cell_type":"code","source":"system_prompt=\"You are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.\"\n\nmy_prompt=\"Do you cover Kafka under this course\"\n\nresponse=apply_chat_template(system_prompt,my_prompt,max_new_tokens=1000)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:05:35.699997Z","iopub.execute_input":"2025-05-05T05:05:35.700503Z","iopub.status.idle":"2025-05-05T05:05:42.536603Z","shell.execute_reply.started":"2025-05-05T05:05:35.700477Z","shell.execute_reply":"2025-05-05T05:05:42.535935Z"}},"outputs":[{"name":"stdout","text":"system\n\nCutting Knowledge Date: December 2023\nToday Date: 05 May 2025\n\nYou are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.user\n\nDo you cover Kafka under this courseassistant\n\nIn the CS 639 Data Management for Data Science course at UW-Madison, we do cover Kafka, but with a focus on its data processing and event-driven architecture.\n\nKafka is a popular distributed streaming platform designed for building real-time data pipelines, event-driven architectures, and microservices. We'll discuss its key concepts, architecture, and use cases, including:\n\n1. **Distributed architecture**: We'll explore how Kafka handles data across multiple nodes, including producers, consumers, and brokers.\n2. **Producer-consumer model**: We'll analyze the producer-consumer model, including how data is sent from producers to brokers, and then consumed by consumers.\n3. **Event-driven architecture**: We'll discuss how Kafka enables event-driven architectures, where data is processed and transformed in real-time.\n4. **Key features**: We'll cover key features such as message ordering, partitioning, and message retention.\n\nTo illustrate these concepts, we'll use a real-world example, such as a social media platform, where we'll demonstrate how Kafka is used to process and store large volumes of data.\n\nWhile Kafka is an essential topic in the course, I want to emphasize that it's not the only focus of the course. We'll also cover other topics, such as:\n\n* Data processing and analytics\n* Data warehousing and ETL\n* Data governance and security\n* Big data and cloud computing\n\nSo, if you have any questions or need help with Kafka, feel free to ask, and I'll do my best to address them!\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"Q2.2 Fine-tune the model on course lecture transcripts with LoRA.","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nfrom peft import LoraConfig\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:05:56.661571Z","iopub.execute_input":"2025-05-05T05:05:56.661913Z","iopub.status.idle":"2025-05-05T05:06:00.935764Z","shell.execute_reply.started":"2025-05-05T05:05:56.661889Z","shell.execute_reply":"2025-05-05T05:06:00.935190Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"mydir = \"./transcripts/transcripts/\"\ntest_ratio = 0.1\n\nfiles = []\nfor i in os.listdir(mydir):\n    print(i)\n    with open(os.path.join(mydir, i), \"r\") as f:\n        files.append(f.read())\n\nprint(f\"Total transcripts found: {len(files)}\")\nsplit_idx = int(len(files) * (test_ratio))\ntest_texts = files[:split_idx]\ntrain_texts = files[split_idx:]\n\nprint(f\"Train samples: {len(train_texts)}\")\nprint(f\"Test samples: {len(test_texts)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:06:59.816686Z","iopub.execute_input":"2025-05-05T05:06:59.816997Z","iopub.status.idle":"2025-05-05T05:06:59.825794Z","shell.execute_reply.started":"2025-05-05T05:06:59.816972Z","shell.execute_reply":"2025-05-05T05:06:59.825053Z"}},"outputs":[{"name":"stdout","text":"5 en-English-CS639_ Relational Database Management Systems (RDBMS).txt\n2 en-English-CS639_ Deployment (Linux Shell).txt\n11 en-English-CS639_ SQL Joins.txt\n8 en-English-CS639_ Relational Algebra (RA).txt\n13 en-English-CS639_ Non-relational databases_ MongoDB.txt\n16 en-English-CS639_ MongoDB Operators.txt\n14 en-English-CS639_ MongoDB on Docker.txt\n3 en-English-CS639_ Deployment (Linux Pipelines).txt\n7 en-English-CS639_ SQL on docker.txt\n1 en-English-CS639_ Course intro.txt\n6.2 en-English-SQL 1_ Creating tables (post fire-alarm).txt\n20 en-English-CS639_ Elasticsearch intro.txt\n17 en-English-CS639_ MongoDB Aggregation.txt\n6.1 en-English-CS639_ SQL 1_ Creating tables (part 1).txt\n15 en-English-CS639_ MongoDB API.txt\n12 en-English-CS639_ SQL window functions.txt\n22 en-English-CS639_ Elasticsearch_ Boosting, highlighting, and aggregations.txt\n18 en-English-CS639_ MongoDB Geospatial Operators.txt\n23 en-English-CS639_ Elasticsearch geo queries + Kibana.txt\n4 en-English-CS639_ Docker.txt\n10 en-English-CS639_ SQL subqueries.txt\n9 en-English-CS639_ Basic SQL queries (partial lecture).txt\n21 en-English-CS639_ Elasticsearch API intro.txt\nTotal transcripts found: 23\nTrain samples: 21\nTest samples: 2\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"train_dataset = Dataset.from_dict({\"text\": train_texts})\ntest_dataset = Dataset.from_dict({\"text\": test_texts})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:07:04.177529Z","iopub.execute_input":"2025-05-05T05:07:04.177866Z","iopub.status.idle":"2025-05-05T05:07:04.200191Z","shell.execute_reply.started":"2025-05-05T05:07:04.177843Z","shell.execute_reply":"2025-05-05T05:07:04.199464Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def tokenize_data(data):\n    tokenized = tokenizer(\n        data[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=512,\n    )\n    # Set the labels to be the same as input_ids for causal language modeling\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\ntokenized_train = train_dataset.map(tokenize_data, batched=True)\ntokenized_test = test_dataset.map(tokenize_data, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:07:04.499437Z","iopub.execute_input":"2025-05-05T05:07:04.500059Z","iopub.status.idle":"2025-05-05T05:07:05.159389Z","shell.execute_reply.started":"2025-05-05T05:07:04.500034Z","shell.execute_reply":"2025-05-05T05:07:05.158552Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3827a57f79524dc3bc6e475277ba4f2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e0714fc1acc4deabc41cb6bd7fa97c0"}},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"tokenized_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:07:06.657297Z","iopub.execute_input":"2025-05-05T05:07:06.658061Z","iopub.status.idle":"2025-05-05T05:07:06.662795Z","shell.execute_reply.started":"2025-05-05T05:07:06.658034Z","shell.execute_reply":"2025-05-05T05:07:06.662062Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 21\n})"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"tokenized_test\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:07:06.970370Z","iopub.execute_input":"2025-05-05T05:07:06.970935Z","iopub.status.idle":"2025-05-05T05:07:06.975406Z","shell.execute_reply.started":"2025-05-05T05:07:06.970912Z","shell.execute_reply":"2025-05-05T05:07:06.974816Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 2\n})"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"from peft import LoraConfig\nfrom transformers import Trainer, TrainingArguments\nfrom trl import SFTTrainer\n\nlora_config = LoraConfig(\n    r=8,\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\n        \"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\"\n    ],\n)\n\ntraining_args = TrainingArguments(\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    num_train_epochs=10,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=3, # optimized\n    learning_rate=1e-4, #optimized\n    fp16=True,\n    logging_steps=1,\n    logging_dir=\"./logs\",\n    output_dir=\"./results\",\n    save_total_limit=2,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"wandb\"\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:07:08.684221Z","iopub.execute_input":"2025-05-05T05:07:08.684949Z","iopub.status.idle":"2025-05-05T05:07:08.713453Z","shell.execute_reply.started":"2025-05-05T05:07:08.684921Z","shell.execute_reply":"2025-05-05T05:07:08.712933Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# Trainer object that takes care of the training process\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_test,\n    args=training_args,\n    peft_config=lora_config,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:07:11.118851Z","iopub.execute_input":"2025-05-05T05:07:11.119618Z","iopub.status.idle":"2025-05-05T05:07:12.169819Z","shell.execute_reply.started":"2025-05-05T05:07:11.119591Z","shell.execute_reply":"2025-05-05T05:07:12.168289Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/21 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db75df4f42cc4f8f9a551b4981e08c5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2604ca2821434b91a9fcb46b91520c12"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"a5ec8b7d493b720df98c3065ec8caae8b3020b72\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:07:12.172107Z","iopub.execute_input":"2025-05-05T05:07:12.172622Z","iopub.status.idle":"2025-05-05T05:07:17.835257Z","shell.execute_reply.started":"2025-05-05T05:07:12.172588Z","shell.execute_reply":"2025-05-05T05:07:17.834628Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlekshmi288\u001b[0m (\u001b[33mlekshmi288-university-of-wisconsin-madison\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"import wandb\nwandb.init(project=\"llm_transcript\") \ntrainer.train()\nwandb.finish() \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:07:17.836273Z","iopub.execute_input":"2025-05-05T05:07:17.836858Z","iopub.status.idle":"2025-05-05T05:09:34.060622Z","shell.execute_reply.started":"2025-05-05T05:07:17.836837Z","shell.execute_reply":"2025-05-05T05:09:34.059866Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250505_050717-jo85zgmd</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript/runs/jo85zgmd' target=\"_blank\">stellar-carrier-8</a></strong> to <a href='https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript' target=\"_blank\">https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript/runs/jo85zgmd' target=\"_blank\">https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript/runs/jo85zgmd</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [70/70 02:05, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.110800</td>\n      <td>3.136825</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.802000</td>\n      <td>3.047174</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.846200</td>\n      <td>2.978147</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.775200</td>\n      <td>2.938261</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.717900</td>\n      <td>2.916264</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.642400</td>\n      <td>2.902479</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.605500</td>\n      <td>2.897469</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.386600</td>\n      <td>2.896417</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.559700</td>\n      <td>2.898170</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.430300</td>\n      <td>2.899535</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–</td></tr><tr><td>eval/mean_token_accuracy</td><td>â–â–‚â–„â–„â–†â–‡â–ˆâ–‡â–ˆâ–ˆ</td></tr><tr><td>eval/num_tokens</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–ˆ</td></tr><tr><td>eval/runtime</td><td>â–„â–â–‚â–‚â–‚â–‡â–‚â–‚â–ˆâ–ˆ</td></tr><tr><td>eval/samples_per_second</td><td>â–…â–ˆâ–‡â–‡â–‡â–‚â–‡â–‡â–â–</td></tr><tr><td>eval/steps_per_second</td><td>â–…â–ˆâ–‡â–‡â–‡â–‚â–‡â–‡â–â–</td></tr><tr><td>train/epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–„â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–„â–„â–…â–„â–…â–…â–…â–…â–…â–„â–…â–†â–…â–†â–†â–†â–†â–‡â–…â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆ</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–†â–‡â–…â–‡â–…â–‡â–…â–†â–†â–…â–„â–„â–„â–„â–„â–…â–„â–…â–„â–ƒâ–„â–„â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–â–â–ƒ</td></tr><tr><td>train/mean_token_accuracy</td><td>â–â–ƒâ–ƒâ–‚â–â–ƒâ–ƒâ–„â–ƒâ–„â–‚â–…â–„â–„â–…â–„â–ƒâ–„â–„â–ƒâ–…â–…â–†â–„â–…â–„â–…â–‡â–…â–†â–†â–…â–…â–†â–†â–ˆâ–…â–‡â–ˆâ–‡</td></tr><tr><td>train/num_tokens</td><td>â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.89954</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.40509</td></tr><tr><td>eval/num_tokens</td><td>107520</td></tr><tr><td>eval/runtime</td><td>0.543</td></tr><tr><td>eval/samples_per_second</td><td>3.683</td></tr><tr><td>eval/steps_per_second</td><td>3.683</td></tr><tr><td>total_flos</td><td>631431978024960.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>70</td></tr><tr><td>train/grad_norm</td><td>0.8639</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.4303</td></tr><tr><td>train/mean_token_accuracy</td><td>0.44618</td></tr><tr><td>train/num_tokens</td><td>107520</td></tr><tr><td>train_loss</td><td>2.73101</td></tr><tr><td>train_runtime</td><td>127.6016</td></tr><tr><td>train_samples_per_second</td><td>1.646</td></tr><tr><td>train_steps_per_second</td><td>0.549</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">stellar-carrier-8</strong> at: <a href='https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript/runs/jo85zgmd' target=\"_blank\">https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript/runs/jo85zgmd</a><br> View project at: <a href='https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript' target=\"_blank\">https://wandb.ai/lekshmi288-university-of-wisconsin-madison/llm_transcript</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250505_050717-jo85zgmd/logs</code>"},"metadata":{}}],"execution_count":46},{"cell_type":"markdown","source":"Q2.3: Test the model after fine-tuning.","metadata":{}},{"cell_type":"code","source":"system_prompt=\"You are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.\"\n\nmy_prompt=\"Do you cover Kafka under this course\"\n\nresponse_finetuned=apply_chat_template(system_prompt,my_prompt,max_new_tokens=1000)\nprint(response_finetuned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:09:34.062100Z","iopub.execute_input":"2025-05-05T05:09:34.062312Z","iopub.status.idle":"2025-05-05T05:09:42.169985Z","shell.execute_reply.started":"2025-05-05T05:09:34.062295Z","shell.execute_reply":"2025-05-05T05:09:42.169184Z"}},"outputs":[{"name":"stdout","text":"system\n\nCutting Knowledge Date: December 2023\nToday Date: 05 May 2025\n\nYou are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.user\n\nDo you cover Kafka under this courseassistant\n\nKafka is actually a key component of the Data Science course at UW-Madison, and it's not covered under the Data Management course. The Data Management course is more focused on the basics of data management, including data types, data structures, data normalization, data integrity, and data management system (DMS) concepts.\n\nKafka, on the other hand, is a distributed streaming platform that provides a scalable and fault-tolerant way to process large amounts of data. It's a crucial part of the Data Science course because it's used to handle high volumes of data that need to be processed in real-time.\n\nIf you're interested in learning more about Kafka, you can definitely take a course on it, such as the \"Kafka for Data Science\" course on Coursera. This course covers the basics of Kafka, including its architecture, its components, and its use cases.\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"Yes! Looks like there is a significant response improvement. The training and testing was based on the topics covered in the course transcripts doesn't include Kafka. So model correctly said its not covered in this curriculum based on the transcripts provided. Its working as expected.","metadata":{}},{"cell_type":"code","source":"system_prompt=\"You are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.\"\n\nmy_prompt=\"How is Elasticsearch different from SQL?\"\n\nresponse_finetuned=apply_chat_template(system_prompt,my_prompt,max_new_tokens=800)\nprint(response_finetuned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:12:55.682683Z","iopub.execute_input":"2025-05-05T05:12:55.683000Z","iopub.status.idle":"2025-05-05T05:13:15.920707Z","shell.execute_reply.started":"2025-05-05T05:12:55.682978Z","shell.execute_reply":"2025-05-05T05:13:15.919945Z"}},"outputs":[{"name":"stdout","text":"system\n\nCutting Knowledge Date: December 2023\nToday Date: 05 May 2025\n\nYou are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.user\n\nHow is Elasticsearch different from SQL?assistant\n\nElasticsearch is a search and analytics engine that is designed to handle large volumes of data. It's not a replacement for SQL databases, but rather a complementary tool that can be used to analyze and query large amounts of data.\n\nHere are some key differences between Elasticsearch and SQL:\n\n1. **Data Storage**: Elasticsearch stores data in a NoSQL document format, whereas SQL databases store data in a relational table format. This means that Elasticsearch is more suitable for storing and querying large volumes of semi-structured or unstructured data.\n\n2. **Querying**: Elasticsearch is designed for search and analytics, whereas SQL is designed for querying and manipulating data. Elasticsearch uses a query language called Elasticsearch Query Language (EQL) to perform queries on data. SQL, on the other hand, uses SQL query language to perform queries on data.\n\n3. **Data Types**: Elasticsearch supports a wide range of data types, including text, number, and date. SQL databases, on the other hand, are limited to a few data types, such as integer, string, and date.\n\n4. **Data Size**: Elasticsearch is designed to handle large volumes of data, whereas SQL databases are limited to storing a fixed amount of data.\n\n5. **Scalability**: Elasticsearch is designed to scale horizontally, which means that it can handle a large number of users and queries at the same time. SQL databases, on the other hand, are limited to scaling vertically, which means that they can handle a large number of users and queries by increasing the number of database instances.\n\n6. **Query Performance**: Elasticsearch query performance is generally better than SQL query performance, especially for complex queries.\n\n7. **Data Analysis**: Elasticsearch is designed to support advanced data analysis, whereas SQL databases are limited to supporting basic data analysis.\n\n8. **Integration**: Elasticsearch is designed to be integrated with other tools and services, such as machine learning, data mining, and data visualization tools. SQL databases, on the other hand, are typically integrated with other tools and services, such as data warehousing, data mining, and data visualization tools.\n\nIn summary, Elasticsearch is a powerful tool for analyzing and querying large volumes of data, whereas SQL is a more traditional tool for querying and manipulating data.\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"my_prompt=\"What are the basic SQL queries?\"\n\nresponse_finetuned=apply_chat_template(system_prompt,my_prompt,max_new_tokens=800)\nprint(response_finetuned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:18:37.886743Z","iopub.execute_input":"2025-05-05T05:18:37.887395Z","iopub.status.idle":"2025-05-05T05:19:08.215806Z","shell.execute_reply.started":"2025-05-05T05:18:37.887373Z","shell.execute_reply":"2025-05-05T05:19:08.215146Z"}},"outputs":[{"name":"stdout","text":"system\n\nCutting Knowledge Date: December 2023\nToday Date: 05 May 2025\n\nYou are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.user\n\nWhat are the basic SQL queries?assistant\n\nAs an instructor for CS 639 Data Management for Data Science course at UW-Madison, I'd like to cover the basic SQL queries. Here are some of the most commonly used queries:\n\n### 1. Selecting Data\n- `SELECT`: Used to select data from the database. It's typically used in combination with `FROM` and `WHERE`.\n- `SELECT column_name(s) FROM table_name WHERE condition;`\n\nExample: `SELECT * FROM customers WHERE age > 18;`\n\n### 2. Filtering Data\n- `WHERE`: Used to filter data based on a condition.\n- `AND`: Used to combine multiple conditions.\n- `OR`: Used to combine multiple conditions.\n- `NOT`: Used to negate a condition.\n\nExample: `SELECT * FROM customers WHERE age > 18 AND country='USA';`\n\n### 3. Sorting and Ranking Data\n- `ORDER BY`: Used to sort data in ascending or descending order.\n- `ASC` or `DESC`: Used to sort data in ascending or descending order.\n- `LIMIT`: Used to limit the number of rows returned.\n\nExample: `SELECT * FROM customers ORDER BY age ASC LIMIT 10;`\n\n### 4. Joining Tables\n- `JOIN`: Used to combine data from two or more tables based on a common column.\n- `INNER JOIN`, `LEFT JOIN`, `RIGHT JOIN`, `FULL JOIN`: Used to combine data from multiple tables based on a common column.\n\nExample: `SELECT * FROM customers JOIN orders ON customers.customer_id=orders.customer_id;`\n\n### 5. Aggregating Data\n- `GROUP BY`: Used to aggregate data based on a column.\n- `HAVING`: Used to filter data based on a condition.\n\nExample: `SELECT AVG(age) FROM customers GROUP BY age HAVING AVG(age) > 18;`\n\n### 6. Subqueries\n- `IN`: Used to filter data based on a subquery.\n- `EXISTS`: Used to filter data based on a subquery.\n\nExample: `SELECT * FROM customers WHERE EXISTS (SELECT 1 FROM orders WHERE customer_id=customers.customer_id);`\n\n### 7. Window Functions\n- `ROW_NUMBER()`, `DENSE_RANK()`, `LAG()`, `Lead()`: Used to perform calculations on a window of rows.\n\nExample: `SELECT *, ROW_NUMBER() OVER (ORDER BY age) FROM customers;`\n\n### 8. Common Table Expressions (CTEs)\n- `CREATE TABLE temp_table AS SELECT * FROM customers;`\n- `SELECT * FROM temp_table;`\n\nExample: `CREATE TABLE temp_table AS SELECT * FROM customers; SELECT * FROM temp_table;`\n\n### 9. Subqueries\n- `IN`, `EXISTS`, `NOT EXISTS`, `ANY`, `ALL`: Used to filter data based on a subquery.\n\nExample: `SELECT * FROM customers WHERE EXISTS (SELECT 1 FROM orders WHERE customer_id=customers.customer_id);`\n\n### 10. Views\n- `CREATE VIEW view_name AS SELECT * FROM table_name;`\n- `SELECT * FROM view_name;`\n\nExample: `CREATE VIEW view_name AS SELECT * FROM customers; SELECT * FROM view_name;`\n\nThese are the basic SQL queries that you should know. As you progress through your course, you'll learn about more advanced queries and how to use them effectively.\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"my_prompt=\"What is LoRA?\"\n\nresponse_finetuned=apply_chat_template(system_prompt,my_prompt,max_new_tokens=800)\nprint(response_finetuned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:21:28.531322Z","iopub.execute_input":"2025-05-05T05:21:28.531674Z","iopub.status.idle":"2025-05-05T05:21:50.990541Z","shell.execute_reply.started":"2025-05-05T05:21:28.531652Z","shell.execute_reply":"2025-05-05T05:21:50.989741Z"}},"outputs":[{"name":"stdout","text":"system\n\nCutting Knowledge Date: December 2023\nToday Date: 05 May 2025\n\nYou are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.user\n\nWhat is LoRAassistant\n\nLoRA (Low-Resolution Audio) is a type of audio compression algorithm that is designed to provide a balance between sound quality and file size. It is primarily used for low-resolution audio formats such as WAV, AIFF, and MP3.\n\nLoRA is based on the idea of representing audio data as a combination of smaller, more representative samples. This is done by using a combination of 16-bit samples and 8-bit samples to represent the audio data. The 8-bit samples are used to represent the audio data, while the 16-bit samples are used to represent the audio data in a way that preserves the higher frequency components of the audio.\n\nThe main advantages of LoRA are:\n\n1. Reduced file size: LoRA can significantly reduce the file size of a compressed audio file compared to other formats like WAV.\n2. Improved sound quality: LoRA can preserve the higher frequency components of the audio, which results in a better sound quality compared to other formats.\n3. Simplicity: LoRA is relatively simple to implement and does not require the use of complex algorithms.\n\nHowever, LoRA also has some disadvantages, such as:\n\n1. Loss of audio information: LoRA can lose some audio information, which results in a loss of audio quality.\n2. Limited dynamic range: LoRA can only represent a limited dynamic range of the audio, which results in a loss of audio information.\n3. Limited resolution: LoRA can only represent a limited resolution of the audio, which results in a loss of audio information.\n\nLoRA is widely used in various applications, including:\n\n1. Music streaming services: LoRA is widely used in music streaming services to compress audio files and reduce the file size.\n2. Podcasting: LoRA is used to compress podcast audio files, which reduces the file size and makes it easier to stream.\n3. Audio editing software: LoRA is used in various audio editing software to compress audio files and improve the sound quality.\n\nHowever, LoRA has some limitations, such as:\n\n1. Lossy compression: LoRA is a lossy compression algorithm, which means that it discards some audio information to reduce the file size.\n2. Limited support: LoRA is not widely supported by all audio devices and software, which can make it difficult to use LoRA in certain applications.\n\nOverall, LoRA is a popular choice for compressing low-resolution audio files, but it has some limitations and limitations that need to be considered when using LoRA.\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"my_prompt=\"What is the weather today in Madison?\"\n\nresponse_finetuned=apply_chat_template(system_prompt,my_prompt,max_new_tokens=800)\nprint(response_finetuned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:23:23.209077Z","iopub.execute_input":"2025-05-05T05:23:23.209678Z","iopub.status.idle":"2025-05-05T05:23:29.725304Z","shell.execute_reply.started":"2025-05-05T05:23:23.209654Z","shell.execute_reply":"2025-05-05T05:23:29.724672Z"}},"outputs":[{"name":"stdout","text":"system\n\nCutting Knowledge Date: December 2023\nToday Date: 05 May 2025\n\nYou are an instructor of CS 639 Data Management for Data Science course at UW-Madison, and are currently answering student questions.user\n\nWhat is the weather today in Madison?assistant\n\nI'm a large language model, I don't have real-time access to current weather conditions. However, I can suggest a few ways for you to find the current weather in Madison.\n\nYou can check the current weather conditions for Madison on websites like:\n\n- National Weather Service (NWS) website: weather.gov\n- National Weather Service (NWS) mobile app\n- Weather.com website\n\nPlease note that the information provided by these sources is usually updated at the time of posting, so you may get the most current information.\n\nIf you're looking for a more general answer, I can tell you that the average temperature in Madison, Wisconsin, during the month of May, is around 45Â°F (7Â°C).\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}